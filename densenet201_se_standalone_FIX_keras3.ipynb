{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "e1f0b19c",
            "metadata": {},
            "source": [
                "# ğŸŒ¾ DenseNet201-SE â€” Paddy Disease Classification\n",
                "\n",
                "Implementasi **DenseNet201 + Squeeze & Excitation (SE) Block** untuk klasifikasi penyakit daun padi.\n",
                "\n",
                "## âœ¨ Keunggulan Notebook Ini\n",
                "\n",
                "| Fitur | Detail |\n",
                "|---|---|\n",
                "| **Arsitektur** | DenseNet201 + SE Block + GAP + Dropout + Softmax |\n",
                "| **Segmentasi** | **GrabCut Auto-Seed** (lebih robust dari HSV) |\n",
                "| **Training** | GrabCut diterapkan di training & inference â€” **no domain shift** |\n",
                "| **Loss** | Focal Loss (Î³=2.0, Î±=0.25) â€” tahan class imbalance |\n",
                "| **Training** | 2-Stage: Freeze Head â†’ Fine-Tuning Backbone |\n",
                "| **Compatibility** | Keras 3 (`@register_keras_serializable`) |\n",
                "\n",
                "## ğŸ“‹ Urutan Menjalankan\n",
                "\n",
                "### Training (dari awal):\n",
                "```\n",
                "Cell 1  â†’ Import library\n",
                "Cell 2  â†’ Konfigurasi (set USE_GRABCUT_TRAINING)\n",
                "Cell 2b â†’ GrabCut utility functions â† WAJIB\n",
                "Cell 3  â†’ Dataset loading\n",
                "Cell 4  â†’ Train/Val split\n",
                "Cell 5  â†’ tf.data pipeline (+ GrabCut opsional)\n",
                "Cell 6  â†’ Custom components (FocalLoss, DenseNetPreprocess)\n",
                "Cell 7  â†’ Build model\n",
                "Cell 8  â†’ Class weights\n",
                "Cell 9  â†’ Plot helper\n",
                "Cell 10 â†’ Stage 1: Train head\n",
                "Cell 11 â†’ Stage 2: Fine-tuning\n",
                "Cell 12 â†’ Evaluasi\n",
                "Cell 13 â†’ Info model\n",
                "```\n",
                "\n",
                "### Inference saja (model sudah ada):\n",
                "```\n",
                "Cell 2b â†’ GrabCut functions â† WAJIB\n",
                "Cell 13a â†’ Load model\n",
                "Cell 14b â†’ Prediksi 1 gambar\n",
                "Cell 14c â†’ Batch prediksi â†’ CSV\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1b78c299",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import random\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as mpatches\n",
                "import seaborn as sns\n",
                "import tensorflow as tf\n",
                "import keras\n",
                "from keras import layers\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.utils.class_weight import compute_class_weight\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "\n",
                "print(\"TensorFlow:\", tf.__version__)\n",
                "print(\"Keras:\", keras.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b569d9a0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 1) Configuration\n",
                "# ----------------------------\n",
                "DATASET_DIR   = \"paddy-disease-classification\"\n",
                "TRAIN_IMG_DIR = os.path.join(DATASET_DIR, \"train_images\")\n",
                "TEST_IMG_DIR  = os.path.join(DATASET_DIR, \"test_images\")\n",
                "SAMPLE_SUB    = os.path.join(DATASET_DIR, \"sample_submission.csv\")\n",
                "\n",
                "OUTPUT_DIR = \"outputs_densenet201_se_standalone\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "SEED = 42\n",
                "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
                "\n",
                "IMG_SIZE   = (224, 224)\n",
                "BATCH_SIZE = 16\n",
                "AUTOTUNE   = tf.data.AUTOTUNE\n",
                "\n",
                "EPOCHS_STAGE1 = 10\n",
                "EPOCHS_STAGE2 = 15\n",
                "\n",
                "LR1           = 1e-3\n",
                "LR2           = 1e-5\n",
                "UNFREEZE_LAST = 30\n",
                "\n",
                "DROPOUT  = 0.4\n",
                "SE_RATIO = 16\n",
                "\n",
                "USE_FOCAL_LOSS = True\n",
                "GAMMA = 2.0\n",
                "ALPHA = 0.25",
                "\n",
                "# GrabCut di training: True=konsisten inference/training, False=RAW\n",
                "USE_GRABCUT_TRAINING = True\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "grabcut_utils",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 2b) GrabCut Auto-Seed Segmentation â€” Didefinisikan Sekali, Dipakai Training & Inference\n",
                "# ----------------------------\n",
                "try:\n",
                "    import cv2\n",
                "    print(f\"[INFO] âœ… OpenCV {cv2.__version__}\")\n",
                "except ImportError:\n",
                "    raise ImportError(\"Install dulu: pip install opencv-python\")\n",
                "import numpy as np\n",
                "\n",
                "# â”€â”€ 1. UTILITAS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "\n",
                "def keep_largest_component(mask_u8):\n",
                "    \"\"\"Ambil komponen putih terbesar.\"\"\"\n",
                "    mask = (mask_u8 > 0).astype(np.uint8)\n",
                "    num, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
                "    if num <= 1: return (mask * 255).astype(np.uint8)\n",
                "    largest = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
                "    return (labels == largest).astype(np.uint8) * 255\n",
                "\n",
                "def refine_mask(mask_u8, close_k=15, open_k=7, close_it=2, open_it=1):\n",
                "    \"\"\"CLOSE (tutup lubang) â†’ OPEN (hapus noise kecil).\"\"\"\n",
                "    m  = (mask_u8 > 0).astype(np.uint8) * 255\n",
                "    kc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (close_k, close_k))\n",
                "    ko = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (open_k,  open_k))\n",
                "    m  = cv2.morphologyEx(m, cv2.MORPH_CLOSE, kc, iterations=close_it)\n",
                "    m  = cv2.morphologyEx(m, cv2.MORPH_OPEN,  ko, iterations=open_it)\n",
                "    return m\n",
                "\n",
                "def crop_by_mask(bgr, mask_u8, pad=18):\n",
                "    \"\"\"Crop pixel-accurate ke bounding box mask.\"\"\"\n",
                "    ys, xs = np.where(mask_u8 > 0)\n",
                "    if len(xs) == 0: return bgr, (0, 0, bgr.shape[1], bgr.shape[0])\n",
                "    h, w = bgr.shape[:2]\n",
                "    x1 = max(0, xs.min()-pad); y1 = max(0, ys.min()-pad)\n",
                "    x2 = min(w-1, xs.max()+pad); y2 = min(h-1, ys.max()+pad)\n",
                "    return bgr[y1:y2+1, x1:x2+1], (x1, y1, x2, y2)\n",
                "\n",
                "# â”€â”€ 2. GRABCUT AUTO-SEED â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "\n",
                "def grabcut_leaf_mask_autoseed(bgr, iters=5):\n",
                "    \"\"\"\n",
                "    GrabCut dengan 3-zona seed otomatis:\n",
                "      Sure FG  â†’ H=[25-95], S>=60, V>=40  (hijau yekin = daun)\n",
                "      Sure BG  â†’ V<=20 | V>=245 | S<=10   (gelap/terang/abu = background)\n",
                "      Unknown  â†’ sisa piksel, GrabCut putuskan via GMM\n",
                "    Fallback   â†’ rect-based jika FG seed < 0.5% gambar.\n",
                "    \"\"\"\n",
                "    h, w  = bgr.shape[:2]\n",
                "    hsv   = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n",
                "    H_ch, S_ch, V_ch = cv2.split(hsv)\n",
                "\n",
                "    sure_fg = ((H_ch>=25)&(H_ch<=95)&(S_ch>=60)&(V_ch>=40)).astype(np.uint8)*255\n",
                "    sure_bg = ((V_ch<=20)|(V_ch>=245)|(S_ch<=10)).astype(np.uint8)*255\n",
                "\n",
                "    k7 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7))\n",
                "    sure_fg = cv2.morphologyEx(sure_fg, cv2.MORPH_OPEN,  k7, iterations=1)\n",
                "    sure_fg = cv2.morphologyEx(sure_fg, cv2.MORPH_CLOSE, k7, iterations=2)\n",
                "    sure_bg = cv2.morphologyEx(sure_bg, cv2.MORPH_OPEN,  k7, iterations=1)\n",
                "\n",
                "    bgdM = np.zeros((1,65), np.float64)\n",
                "    fgdM = np.zeros((1,65), np.float64)\n",
                "\n",
                "    if cv2.countNonZero(sure_fg) < 0.005*(h*w):\n",
                "        # Fallback: rect tengah\n",
                "        gc = np.zeros((h,w), np.uint8)\n",
                "        cv2.grabCut(bgr, gc, (10,10,w-20,h-20), bgdM, fgdM, iters, cv2.GC_INIT_WITH_RECT)\n",
                "    else:\n",
                "        gc = np.full((h,w), cv2.GC_PR_BGD, np.uint8)\n",
                "        gc[sure_bg>0] = cv2.GC_BGD\n",
                "        gc[sure_fg>0] = cv2.GC_FGD\n",
                "        cv2.grabCut(bgr, gc, None, bgdM, fgdM, iters, cv2.GC_INIT_WITH_MASK)\n",
                "\n",
                "    out = np.where((gc==cv2.GC_FGD)|(gc==cv2.GC_PR_FGD), 255, 0).astype(np.uint8)\n",
                "    out = refine_mask(out, close_k=21, open_k=9, close_it=2, open_it=1)\n",
                "    out = keep_largest_component(out)\n",
                "    return out\n",
                "\n",
                "# â”€â”€ 3. PIPELINE LENGKAP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "\n",
                "def preprocess_leaf_grabcut(bgr, target_size=(224,224), pad=18):\n",
                "    \"\"\"\n",
                "    GrabCut â†’ BG hitam â¬› â†’ crop ROI â†’ resize â†’ RGB float32.\n",
                "    Dipakai di: training pipeline + inference.\n",
                "    \"\"\"\n",
                "    leaf_mask  = grabcut_leaf_mask_autoseed(bgr, iters=5)\n",
                "    leaf_only  = cv2.bitwise_and(bgr, bgr, mask=leaf_mask)\n",
                "    crop, bbox = crop_by_mask(leaf_only, leaf_mask, pad=pad)\n",
                "    crop_rsz   = cv2.resize(crop, target_size, interpolation=cv2.INTER_AREA)\n",
                "    rgb_model  = cv2.cvtColor(crop_rsz, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
                "    return rgb_model, leaf_mask, leaf_only, bbox\n",
                "\n",
                "# â”€â”€ 4. TF.NUMPY_FUNCTION WRAPPER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "\n",
                "def _tf_grabcut_map(image, label):\n",
                "    \"\"\"\n",
                "    Wrapper tf.numpy_function untuk integrasi ke tf.data.\n",
                "    Dipanggil lewat .map() saat training/validasi.\n",
                "    Input : image tensor (H,W,3) float32 0..255 (RGB dari decode_image)\n",
                "    Output: image tensor (224,224,3) float32 â€” sudah tersegmentasi GrabCut\n",
                "    \"\"\"\n",
                "    def _np_fn(img_np):\n",
                "        img_np = np.clip(img_np, 0, 255).astype(np.uint8)\n",
                "        bgr    = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
                "        rgb, _, _, _ = preprocess_leaf_grabcut(bgr, target_size=IMG_SIZE)\n",
                "        return rgb.astype(np.float32)\n",
                "    out = tf.numpy_function(_np_fn, [image], Tout=tf.float32)\n",
                "    out.set_shape([IMG_SIZE[0], IMG_SIZE[1], 3])\n",
                "    return out, label\n",
                "\n",
                "print(\"âœ… GrabCut Auto-Seed functions siap.\")\n",
                "print(f\"   USE_GRABCUT_TRAINING = {USE_GRABCUT_TRAINING}\")\n",
                "print(\"   True  â†’ training + inference pakai GrabCut (NO domain shift)\")\n",
                "print(\"   False â†’ training RAW, inference GrabCut (ada domain shift)\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd3a080f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 2) Dynamic Dataset Loading (Folder Scan)\n",
                "# ----------------------------\n",
                "filepaths = []\n",
                "labels    = []\n",
                "\n",
                "classes = sorted(os.listdir(TRAIN_IMG_DIR))\n",
                "classes = [c for c in classes if os.path.isdir(os.path.join(TRAIN_IMG_DIR, c))]\n",
                "print(f\"[INFO] Classes found ({len(classes)}): {classes}\")\n",
                "\n",
                "class_counts = {}\n",
                "for label in classes:\n",
                "    class_dir = os.path.join(TRAIN_IMG_DIR, label)\n",
                "    images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
                "    class_counts[label] = len(images)\n",
                "    for img in images:\n",
                "        filepaths.append(os.path.join(class_dir, img))\n",
                "        labels.append(label)\n",
                "\n",
                "df = pd.DataFrame({'filepath': filepaths, 'label': labels})\n",
                "\n",
                "print(f\"\\n[INFO] Total Dataset: {len(df)} images\")\n",
                "print(\"Distribution per class:\")\n",
                "print(pd.Series(class_counts))\n",
                "\n",
                "class_names  = sorted(df['label'].unique().tolist())\n",
                "num_classes  = len(class_names)\n",
                "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
                "\n",
                "with open(os.path.join(OUTPUT_DIR, \"class_names.json\"), \"w\") as f:\n",
                "    json.dump(class_names, f, indent=2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28e19e30",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 3) Train/Val Split (Stratified 90:10)\n",
                "# ----------------------------\n",
                "train_df, val_df = train_test_split(\n",
                "    df, test_size=0.10, random_state=SEED, stratify=df[\"label\"]\n",
                ")\n",
                "train_df = train_df.reset_index(drop=True)\n",
                "val_df   = val_df.reset_index(drop=True)\n",
                "\n",
                "print(f\"Training Set   : {len(train_df)} images\")\n",
                "print(f\"Validation Set : {len(val_df)} images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d40f37d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 4) tf.data Input Pipeline (opsional GrabCut preprocessing)\n",
                "# ----------------------------\n",
                "def decode_image(path):\n",
                "    img = tf.io.read_file(path)\n",
                "    img = tf.image.decode_jpeg(img, channels=3)\n",
                "    img = tf.image.resize(img, IMG_SIZE, method=\"bilinear\")\n",
                "    return tf.cast(img, tf.float32)\n",
                "\n",
                "def process_path(path, label_idx):\n",
                "    return decode_image(path), tf.one_hot(label_idx, num_classes)\n",
                "\n",
                "def make_ds(dataframe, training=True):\n",
                "    paths  = dataframe[\"filepath\"].values\n",
                "    idxs   = dataframe[\"label\"].map(class_to_idx).values.astype('int32')\n",
                "    ds     = tf.data.Dataset.from_tensor_slices((paths, idxs))\n",
                "    if training:\n",
                "        ds = ds.shuffle(buffer_size=min(len(dataframe), 5000), seed=SEED)\n",
                "    ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
                "    if USE_GRABCUT_TRAINING:\n",
                "        # Terapkan GrabCut ke setiap gambar â€” konsisten dengan inference\n",
                "        ds = ds.map(_tf_grabcut_map, num_parallel_calls=AUTOTUNE)\n",
                "    return ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
                "\n",
                "train_ds = make_ds(train_df, training=True)\n",
                "val_ds   = make_ds(val_df,   training=False)\n",
                "\n",
                "mode = 'GrabCut Auto-Seed' if USE_GRABCUT_TRAINING else 'RAW (tanpa segmentasi)'\n",
                "print(f\"[INFO] Training pipeline : {mode}\")\n",
                "print(f\"[INFO] Train batches     : {len(train_ds)}\")\n",
                "print(f\"[INFO] Val   batches     : {len(val_ds)}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7e8169ce",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 5) Keras 3 Serializable Components\n",
                "# ----------------------------\n",
                "@keras.saving.register_keras_serializable(package=\"custom\")\n",
                "class FocalLoss(tf.keras.losses.Loss):\n",
                "    def __init__(self, gamma=2.0, alpha=0.25, from_logits=False, name=\"focal_loss\"):\n",
                "        super().__init__(name=name)\n",
                "        self.gamma = gamma\n",
                "        self.alpha = alpha\n",
                "        self.from_logits = from_logits\n",
                "\n",
                "    def call(self, y_true, y_pred):\n",
                "        y_true = tf.cast(y_true, tf.float32)\n",
                "        if self.from_logits:\n",
                "            y_pred = tf.nn.softmax(y_pred)\n",
                "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
                "        ce     = -y_true * tf.math.log(y_pred)\n",
                "        weight = self.alpha * tf.pow(1.0 - y_pred, self.gamma)\n",
                "        return tf.reduce_sum(weight * ce, axis=-1)\n",
                "\n",
                "    def get_config(self):\n",
                "        return {\"gamma\": self.gamma, \"alpha\": self.alpha, \"from_logits\": self.from_logits, \"name\": self.name}\n",
                "\n",
                "@keras.saving.register_keras_serializable(package=\"custom\")\n",
                "class DenseNetPreprocess(tf.keras.layers.Layer):\n",
                "    def call(self, x):\n",
                "        return tf.keras.applications.densenet.preprocess_input(x)\n",
                "    def get_config(self):\n",
                "        return {}\n",
                "\n",
                "LOSS_FN = FocalLoss(gamma=GAMMA, alpha=ALPHA) if USE_FOCAL_LOSS else \"categorical_crossentropy\"\n",
                "print(\"Loss Function:\", LOSS_FN.name if hasattr(LOSS_FN, 'name') else LOSS_FN)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fcd69dbd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 6) Model Architecture (DenseNet201 + SE Block)\n",
                "# ----------------------------\n",
                "def se_block(x, ratio=16, name=\"se\"):\n",
                "    c  = int(x.shape[-1])\n",
                "    se = tf.keras.layers.GlobalAveragePooling2D(name=f\"{name}_gap\")(x)\n",
                "    se = tf.keras.layers.Dense(max(1, c // ratio), activation=\"relu\",    name=f\"{name}_fc1\")(se)\n",
                "    se = tf.keras.layers.Dense(c,                  activation=\"sigmoid\", name=f\"{name}_fc2\")(se)\n",
                "    se = tf.keras.layers.Reshape((1, 1, c),                              name=f\"{name}_reshape\")(se)\n",
                "    return tf.keras.layers.Multiply(name=f\"{name}_scale\")([x, se])\n",
                "\n",
                "def build_densenet201_se(num_classes, dropout=DROPOUT, se_ratio=SE_RATIO, name=\"DenseNet201_SE\"):\n",
                "    backbone = tf.keras.applications.DenseNet201(\n",
                "        include_top=False, weights=\"imagenet\",\n",
                "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
                "    )\n",
                "    backbone.trainable = False\n",
                "\n",
                "    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
                "    x   = DenseNetPreprocess(name=\"densenet_preprocess\")(inp)\n",
                "    x   = backbone(x, training=False)\n",
                "    x   = se_block(x, ratio=se_ratio, name=\"se_block\")\n",
                "    x   = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
                "    x   = tf.keras.layers.Dropout(dropout)(x)\n",
                "    out = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
                "    return tf.keras.Model(inp, out, name=name)\n",
                "\n",
                "model = build_densenet201_se(num_classes)\n",
                "model.compile(optimizer=tf.keras.optimizers.Adam(LR1), loss=LOSS_FN, metrics=[\"accuracy\"])\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "da4b1224",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 7) Class Weights\n",
                "# ----------------------------\n",
                "classes_idx  = train_df[\"label\"].map(class_to_idx).values\n",
                "cw           = compute_class_weight(class_weight=\"balanced\", classes=np.unique(classes_idx), y=classes_idx)\n",
                "class_weight = {i: float(w) for i, w in enumerate(cw)}\n",
                "print(\"Class Weights:\", class_weight)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "097c14ae",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 8) Train & Eval Plot Helper\n",
                "# ----------------------------\n",
                "def plot_history(history, stage_name=\"Training\"):\n",
                "    acc      = history.history['accuracy']\n",
                "    val_acc  = history.history['val_accuracy']\n",
                "    loss     = history.history['loss']\n",
                "    val_loss = history.history['val_loss']\n",
                "    epochs   = range(1, len(acc) + 1)\n",
                "\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
                "    fig.suptitle(f\"Train Evaluation â€” {stage_name}\", fontsize=14, fontweight='bold')\n",
                "\n",
                "    ax1.plot(epochs, acc,     'bo-', label='Train Accuracy')\n",
                "    ax1.plot(epochs, val_acc, 'ro-', label='Val Accuracy')\n",
                "    ax1.set_title('Accuracy'); ax1.set_xlabel('Epoch'); ax1.set_ylabel('Accuracy')\n",
                "    ax1.legend(); ax1.grid(True, alpha=0.3)\n",
                "\n",
                "    ax2.plot(epochs, loss,     'bo-', label='Train Loss')\n",
                "    ax2.plot(epochs, val_loss, 'ro-', label='Val Loss')\n",
                "    ax2.set_title('Loss'); ax2.set_xlabel('Epoch'); ax2.set_ylabel('Loss')\n",
                "    ax2.legend(); ax2.grid(True, alpha=0.3)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c532eaaf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 9) STAGE 1: Train Head Only\n",
                "# ----------------------------\n",
                "checkpoint_path = os.path.join(OUTPUT_DIR, \"best_stage1.keras\")\n",
                "\n",
                "callbacks_stage1 = [\n",
                "    tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
                "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1)\n",
                "]\n",
                "\n",
                "print(\"\\nğŸš€ Starting Stage 1 Training (Head Only)...\")\n",
                "history1 = model.fit(\n",
                "    train_ds, validation_data=val_ds,\n",
                "    epochs=EPOCHS_STAGE1, callbacks=callbacks_stage1, class_weight=class_weight\n",
                ")\n",
                "plot_history(history1, \"Stage 1 â€” Head Training\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43e920b7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 10) STAGE 2: Fine Tuning\n",
                "# ----------------------------\n",
                "model = tf.keras.models.load_model(\n",
                "    checkpoint_path,\n",
                "    custom_objects={\"FocalLoss\": FocalLoss, \"DenseNetPreprocess\": DenseNetPreprocess}\n",
                ")\n",
                "\n",
                "backbone = model.layers[2]\n",
                "backbone.trainable = True\n",
                "\n",
                "for layer in backbone.layers[:-UNFREEZE_LAST]:\n",
                "    layer.trainable = False\n",
                "for layer in backbone.layers:\n",
                "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
                "        layer.trainable = False\n",
                "\n",
                "model.compile(optimizer=tf.keras.optimizers.Adam(LR2), loss=LOSS_FN, metrics=[\"accuracy\"])\n",
                "\n",
                "final_checkpoint_path = os.path.join(OUTPUT_DIR, \"densenet201_se_final.keras\")\n",
                "\n",
                "callbacks_stage2 = [\n",
                "    tf.keras.callbacks.ModelCheckpoint(final_checkpoint_path, monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
                "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True, verbose=1),\n",
                "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1)\n",
                "]\n",
                "\n",
                "print(f\"\\nğŸš€ Starting Stage 2 Fine-Tuning (Unfreezing last {UNFREEZE_LAST} layers)...\")\n",
                "history2 = model.fit(\n",
                "    train_ds, validation_data=val_ds,\n",
                "    epochs=EPOCHS_STAGE2, callbacks=callbacks_stage2, class_weight=class_weight\n",
                ")\n",
                "plot_history(history2, \"Stage 2 â€” Fine Tuning\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "351192f9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 11) Evaluasi: Confusion Matrix & Report\n",
                "# ----------------------------\n",
                "best_model = tf.keras.models.load_model(\n",
                "    final_checkpoint_path,\n",
                "    custom_objects={\"FocalLoss\": FocalLoss, \"DenseNetPreprocess\": DenseNetPreprocess}\n",
                ")\n",
                "\n",
                "print(\"\\nğŸ“Š Generating Evaluation Metrics...\")\n",
                "\n",
                "y_pred, y_true = [], []\n",
                "for bx, by in val_ds:\n",
                "    probs = best_model.predict(bx, verbose=0)\n",
                "    y_pred.extend(np.argmax(probs, axis=1))\n",
                "    y_true.extend(np.argmax(by.numpy(), axis=1))\n",
                "\n",
                "print(\"\\nâœ… Classification Report:\")\n",
                "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
                "\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "plt.figure(figsize=(11, 8))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
                "plt.xlabel('Predicted'); plt.ylabel('Ground Truth')\n",
                "plt.title('Confusion Matrix â€” Validation Set')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4e83b055",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 12) Save Model\n",
                "# ----------------------------\n",
                "print(\"\\nğŸ’¾ Model saved successfully at:\")\n",
                "print(f\"   - {final_checkpoint_path}\")\n",
                "print(\"   (Gunakan file ini untuk inference atau deployment)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "inf_md_001",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸ” 13) Inference Helper\n",
                "\n",
                "Gunakan bagian ini untuk **menguji model** pada gambar baru.\n",
                "**Tidak perlu training ulang** â€” cukup load model yang sudah tersimpan.\n",
                "\n",
                "> âš ï¸ **Cell 2b (GrabCut functions) WAJIB dijalankan dulu** sebelum Cell 13a.\n",
                "\n",
                "| Cell | Fungsi |\n",
                "|:---:|---|\n",
                "| **13a** | Load library + model + class names |\n",
                "| **13b** | Prediksi 1 gambar (tanpa segmentasi â€” baseline) |\n",
                "| **13c** | Batch prediksi folder `test_images/` â†’ CSV (tanpa segmentasi) |\n",
                "\n",
                "Untuk prediksi **dengan GrabCut segmentation**, gunakan **Section 14** (Cell 14b / 14c).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "inf_code_001",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 13a) Load Model + Library untuk Inference\n",
                "# WAJIB dijalankan pertama sebelum Cell 13b / 13c / 14b / 14c\n",
                "# ----------------------------\n",
                "import os\n",
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as mpatches\n",
                "import tensorflow as tf\n",
                "import keras\n",
                "\n",
                "# ============================================================\n",
                "# AUTO-DETECT path model â€” bekerja di CWD manapun\n",
                "# ============================================================\n",
                "def _find_model_path():\n",
                "    \"\"\"Cari file model .keras secara otomatis dari beberapa lokasi kandidat.\"\"\"\n",
                "    # Folder project (absolut, diambil dari lokasi notebook ini)\n",
                "    base_candidates = [\n",
                "        os.getcwd(),\n",
                "        r\"c:\\Users\\azzik\\Documents\\Tugas Akhir\\Skripsi\\Model DenseNet-201\",\n",
                "    ]\n",
                "    model_subpaths = [\n",
                "        os.path.join(\"model\", \"outputs_densenet201_se_standalone\", \"densenet201_se_final.keras\"),\n",
                "        os.path.join(\"outputs_densenet201_se_standalone\", \"densenet201_se_final.keras\"),\n",
                "    ]\n",
                "    for base in base_candidates:\n",
                "        for sub in model_subpaths:\n",
                "            candidate = os.path.join(base, sub)\n",
                "            if os.path.exists(candidate):\n",
                "                return candidate\n",
                "    return None\n",
                "\n",
                "def _find_class_names_path():\n",
                "    \"\"\"Cari file class_names.json secara otomatis.\"\"\"\n",
                "    base_candidates = [\n",
                "        os.getcwd(),\n",
                "        r\"c:\\Users\\azzik\\Documents\\Tugas Akhir\\Skripsi\\Model DenseNet-201\",\n",
                "    ]\n",
                "    cn_subpaths = [\n",
                "        os.path.join(\"model\", \"outputs_densenet201_se_standalone\", \"class_names.json\"),\n",
                "        os.path.join(\"outputs_densenet201_se_standalone\", \"class_names.json\"),\n",
                "        \"class_names.json\",\n",
                "    ]\n",
                "    for base in base_candidates:\n",
                "        for sub in cn_subpaths:\n",
                "            candidate = os.path.join(base, sub)\n",
                "            if os.path.exists(candidate):\n",
                "                return candidate\n",
                "    return None\n",
                "\n",
                "# Resolusi path\n",
                "print(f\"[INFO] Working Directory : {os.getcwd()}\")\n",
                "\n",
                "MODEL_PATH       = _find_model_path()\n",
                "CLASS_NAMES_PATH = _find_class_names_path()\n",
                "CUSTOM_TEST_DIR  = \"test_images\"   # Folder gambar uji (di LUAR paddy-disease-classification)\n",
                "IMG_SIZE         = (224, 224)\n",
                "\n",
                "if MODEL_PATH is None:\n",
                "    raise FileNotFoundError(\n",
                "        \"âŒ File model tidak ditemukan!\\n\"\n",
                "        \"   Pastikan file 'densenet201_se_final.keras' ada di:\\n\"\n",
                "        \"   model/outputs_densenet201_se_standalone/densenet201_se_final.keras\"\n",
                "    )\n",
                "if CLASS_NAMES_PATH is None:\n",
                "    raise FileNotFoundError(\n",
                "        \"âŒ File class_names.json tidak ditemukan!\\n\"\n",
                "        \"   Pastikan file ada di folder output yang sama dengan model.\"\n",
                "    )\n",
                "\n",
                "print(f\"[INFO] Model ditemukan  : {MODEL_PATH}\")\n",
                "print(f\"[INFO] Classes path     : {CLASS_NAMES_PATH}\")\n",
                "\n",
                "# Re-register custom components\n",
                "@keras.saving.register_keras_serializable(package=\"custom\")\n",
                "class FocalLoss(tf.keras.losses.Loss):\n",
                "    def __init__(self, gamma=2.0, alpha=0.25, from_logits=False, name=\"focal_loss\"):\n",
                "        super().__init__(name=name)\n",
                "        self.gamma = gamma; self.alpha = alpha; self.from_logits = from_logits\n",
                "    def call(self, y_true, y_pred):\n",
                "        if self.from_logits: y_pred = tf.nn.softmax(y_pred)\n",
                "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
                "        ce = -tf.cast(y_true, tf.float32) * tf.math.log(y_pred)\n",
                "        return tf.reduce_sum(self.alpha * tf.pow(1.0 - y_pred, self.gamma) * ce, axis=-1)\n",
                "    def get_config(self):\n",
                "        return {\"gamma\": self.gamma, \"alpha\": self.alpha, \"from_logits\": self.from_logits, \"name\": self.name}\n",
                "\n",
                "@keras.saving.register_keras_serializable(package=\"custom\")\n",
                "class DenseNetPreprocess(tf.keras.layers.Layer):\n",
                "    def call(self, x): return tf.keras.applications.densenet.preprocess_input(x)\n",
                "    def get_config(self): return {}\n",
                "\n",
                "CUSTOM_OBJ = {\"FocalLoss\": FocalLoss, \"DenseNetPreprocess\": DenseNetPreprocess}\n",
                "\n",
                "# Load model\n",
                "print(f\"\\n[INFO] Loading model...\")\n",
                "inf_model = tf.keras.models.load_model(MODEL_PATH, custom_objects=CUSTOM_OBJ)\n",
                "print(\"[INFO] âœ… Model berhasil di-load!\")\n",
                "\n",
                "# Load class names\n",
                "with open(CLASS_NAMES_PATH, \"r\") as f:\n",
                "    class_names_inf = json.load(f)\n",
                "\n",
                "print(f\"[INFO] Classes ({len(class_names_inf)}): {class_names_inf}\")\n",
                "print(f\"[INFO] Test Images Dir  : '{CUSTOM_TEST_DIR}'\")\n",
                "print(\"\\nâœ… Siap! Jalankan Cell 13b, 13c, 14b, atau 14c.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "inf_md_002",
            "metadata": {},
            "source": [
                "### 13b. Prediksi 1 Gambar â€” Tanpa Segmentasi (Baseline)\n",
                "\n",
                "Digunakan untuk **membandingkan** hasil dengan/tanpa GrabCut segmentation.\n",
                "Ubah `IMAGE_FILE` ke nama gambar di folder `test_images/`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "inf_code_002",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 13b) Prediksi SATU Gambar (Tanpa Segmentasi)\n",
                "# ----------------------------\n",
                "def predict_single_image(model, class_names, image_path, img_size=(224, 224)):\n",
                "    import matplotlib.patches as mpatches\n",
                "    if not os.path.exists(image_path):\n",
                "        raise FileNotFoundError(f\"âŒ Gambar tidak ditemukan: '{image_path}'\")\n",
                "\n",
                "    img_raw     = tf.io.read_file(image_path)\n",
                "    img         = tf.image.decode_image(img_raw, channels=3, expand_animations=False)\n",
                "    img_display = img.numpy().astype('uint8')\n",
                "    img         = tf.image.resize(img, img_size, method=\"bilinear\")\n",
                "    img         = tf.cast(img, tf.float32)\n",
                "    img         = tf.expand_dims(img, axis=0)\n",
                "\n",
                "    probs      = model.predict(img, verbose=0)[0]\n",
                "    pred_idx   = int(np.argmax(probs))\n",
                "    pred_label = class_names[pred_idx]\n",
                "    confidence = float(probs[pred_idx])\n",
                "\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    fig.suptitle(\"Prediksi DenseNet201-SE (Tanpa Segmentasi)\", fontsize=13, fontweight='bold')\n",
                "    ax1.imshow(img_display); ax1.axis('off')\n",
                "    color = '#27ae60' if confidence >= 0.80 else ('#e67e22' if confidence >= 0.50 else '#e74c3c')\n",
                "    ax1.set_title(f\"ğŸ·ï¸  {pred_label.replace('_',' ').title()}\\nConfidence: {confidence*100:.2f}%\",\n",
                "                  fontsize=12, fontweight='bold', color=color)\n",
                "    colors = ['#27ae60' if i == pred_idx else '#3498db' for i in range(len(class_names))]\n",
                "    bars   = ax2.barh(class_names, probs * 100, color=colors, edgecolor='white', height=0.6)\n",
                "    ax2.set_xlabel('Probability (%)'); ax2.set_title('Distribusi Probabilitas')\n",
                "    ax2.set_xlim(0, 105)\n",
                "    for bar, val in zip(bars, probs):\n",
                "        ax2.text(val * 100 + 0.8, bar.get_y() + bar.get_height() / 2,\n",
                "                 f'{val*100:.1f}%', va='center', fontsize=9)\n",
                "    ax2.legend(handles=[mpatches.Patch(color='#27ae60', label='Predicted'),\n",
                "                        mpatches.Patch(color='#3498db', label='Others')], loc='lower right')\n",
                "    ax2.grid(axis='x', alpha=0.3)\n",
                "    plt.tight_layout(); plt.show()\n",
                "\n",
                "    print(f\"\\nğŸ¯ Kelas: {pred_label} | Confidence: {confidence*100:.2f}%\")\n",
                "    return pred_label, confidence, probs\n",
                "\n",
                "IMAGE_FILE = \"image.png\"   # â† Ganti nama file\n",
                "IMAGE_PATH = os.path.join(CUSTOM_TEST_DIR, IMAGE_FILE)\n",
                "pred_label, confidence, all_probs = predict_single_image(inf_model, class_names_inf, IMAGE_PATH)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "inf_md_003",
            "metadata": {},
            "source": [
                "### 13c. Batch Prediksi `test_images/` â†’ CSV â€” Tanpa Segmentasi (Baseline)\n",
                "\n",
                "Hasilnya disimpan ke `test_images/prediction_results.csv`.\n",
                "Bandingkan dengan `prediction_results_grabcut.csv` dari Cell 14c.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "inf_code_003",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 13c) Batch Prediction â†’ CSV (Tanpa Segmentasi)\n",
                "# ----------------------------\n",
                "def predict_all_in_folder(model, class_names, test_dir, img_size=(224, 224), batch_size=16):\n",
                "    valid_ext = ('.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG')\n",
                "    all_files = sorted([f for f in os.listdir(test_dir) if f.endswith(valid_ext)])\n",
                "    if not all_files:\n",
                "        print(f\"[WARN] Tidak ada gambar di '{test_dir}'.\"); return None\n",
                "    print(f\"[INFO] Ditemukan {len(all_files)} gambar\")\n",
                "    test_paths = [os.path.join(test_dir, f) for f in all_files]\n",
                "\n",
                "    def load_img(path):\n",
                "        raw = tf.io.read_file(path)\n",
                "        img = tf.image.decode_image(raw, channels=3, expand_animations=False)\n",
                "        img = tf.image.resize(img, img_size, method=\"bilinear\")\n",
                "        return tf.cast(img, tf.float32)\n",
                "\n",
                "    test_ds = (tf.data.Dataset.from_tensor_slices(test_paths)\n",
                "               .map(load_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
                "               .batch(batch_size).prefetch(tf.data.AUTOTUNE))\n",
                "\n",
                "    print(\"[INFO] Running inference...\")\n",
                "    all_probs   = np.concatenate([model.predict(b, verbose=0) for b in test_ds], axis=0)\n",
                "    pred_idx    = np.argmax(all_probs, axis=1)\n",
                "    pred_labels = [class_names[i] for i in pred_idx]\n",
                "    confs       = [round(float(all_probs[i, pred_idx[i]]), 4) for i in range(len(pred_idx))]\n",
                "\n",
                "    result_df = pd.DataFrame({\"filename\": all_files, \"predicted_label\": pred_labels, \"confidence\": confs})\n",
                "    out_csv   = os.path.join(test_dir, \"prediction_results.csv\")\n",
                "    result_df.to_csv(out_csv, index=False)\n",
                "    print(f\"[INFO] âœ… Hasil disimpan: {out_csv}\")\n",
                "\n",
                "    pd.Series(pred_labels).value_counts().sort_index().plot(\n",
                "        kind='bar', color='#3498db', edgecolor='white', figsize=(10, 4))\n",
                "    plt.title('Distribusi Prediksi'); plt.xlabel('Kelas'); plt.ylabel('Jumlah')\n",
                "    plt.xticks(rotation=30, ha='right'); plt.grid(axis='y', alpha=0.3)\n",
                "    plt.tight_layout(); plt.show()\n",
                "    print(result_df.to_string(index=False))\n",
                "    return result_df\n",
                "\n",
                "result_df = predict_all_in_folder(inf_model, class_names_inf, CUSTOM_TEST_DIR)\n",
                "if result_df is not None: result_df.head(20)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "seg_md_001",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸƒ 14) Leaf Segmentation + Inference â€” GrabCut Auto-Seed\n",
                "\n",
                "**Mengapa GrabCut lebih baik dari HSV biasa?**\n",
                "\n",
                "| | HSV Basic | GrabCut Auto-Seed |\n",
                "|---|:---:|:---:|\n",
                "| Daun sakit (pucat/kuning) | âŒ Tidak terdeteksi | âœ… Terdeteksi |\n",
                "| Background hijau kompleks | âŒ Ikut terdeteksi | âœ… Terbuang |\n",
                "| Domain shift trainingâ†”inference | âŒ Ada | âœ… Tidak ada |\n",
                "| Kecepatan | âš¡âš¡âš¡ | âš¡ |\n",
                "\n",
                "**Strategi GrabCut Auto-Seed (3 Zona):**\n",
                "\n",
                "```\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚  Sure FG  â†’ H=[25-95], Sâ‰¥60, Vâ‰¥40              â”‚\n",
                "â”‚             Pasti daun (hijau jelas)             â”‚\n",
                "â”‚  Sure BG  â†’ Vâ‰¤20 | Vâ‰¥245 | Sâ‰¤10               â”‚\n",
                "â”‚             Pasti background (gelap/abu/putih)   â”‚\n",
                "â”‚  Unknown  â†’ Sisa piksel                         â”‚\n",
                "â”‚             GrabCut putuskan via GMM (otomatis)  â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "Fallback: jika Sure FG < 0.5% gambar â†’ rect-based GrabCut\n",
                "```\n",
                "\n",
                "**Pipeline:**\n",
                "```\n",
                "Gambar asli\n",
                "  â†’ grabcut_leaf_mask_autoseed()  â† 3-zona seed\n",
                "  â†’ refine_mask()                 â† CLOSE+OPEN morph\n",
                "  â†’ keep_largest_component()      â† blob daun terbesar\n",
                "  â†’ bitwise_and()                 â† BG = HITAM â¬›\n",
                "  â†’ crop_by_mask()               â† crop ketat ke daun\n",
                "  â†’ resize(224,224) â†’ RGB         â† input model\n",
                "```\n",
                "\n",
                "| Cell | Fungsi |\n",
                "|:---:|---|\n",
                "| **14b** | Prediksi 1 gambar â€” **4 panel** (Asli \\| Mask \\| BG Hitam \\| Input Model) |\n",
                "| **14c** | Batch prediksi semua gambar â†’ `prediction_results_grabcut.csv` |\n",
                "\n",
                "> âš ï¸ **Prasyarat:** `Cell 2b` (GrabCut functions) + `Cell 13a` (load model) harus dijalankan dulu.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "seg_code_001",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 14b) Prediksi 1 Gambar â€” GrabCut Auto-Seed\n",
                "# ----------------------------\n",
                "# âš™ï¸ UBAH DI SINI\n",
                "IMAGE_FILE       = \"image.png\"   # nama file di test_images/\n",
                "USE_SEGMENTATION = True          # True = GrabCut | False = tanpa segmentasi\n",
                "# âš™ï¸\n",
                "\n",
                "IMAGE_PATH = os.path.join(CUSTOM_TEST_DIR, IMAGE_FILE)\n",
                "if not os.path.exists(IMAGE_PATH):\n",
                "    raise FileNotFoundError(f\"âŒ File tidak ada: {IMAGE_PATH}\")\n",
                "\n",
                "print(f\"File : {IMAGE_PATH}\")\n",
                "print(f\"Mode : {'GrabCut Auto-Seed' if USE_SEGMENTATION else 'Tanpa Segmentasi'}\")\n",
                "\n",
                "if USE_SEGMENTATION:\n",
                "    bgr_in = cv2.imread(IMAGE_PATH)\n",
                "    rgb_input, leaf_mask, leaf_only, bbox = preprocess_leaf_grabcut(bgr_in, IMG_SIZE)\n",
                "\n",
                "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
                "    fig.suptitle(\"GrabCut Auto-Seed â€” Hanya Daun, Background Hitam â¬›\",\n",
                "                 fontsize=13, fontweight=\"bold\")\n",
                "    axes[0].imshow(cv2.cvtColor(bgr_in, cv2.COLOR_BGR2RGB))\n",
                "    axes[0].set_title(\"1ï¸âƒ£  Gambar Asli\");            axes[0].axis(\"off\")\n",
                "    axes[1].imshow(leaf_mask, cmap=\"Greens\")\n",
                "    axes[1].set_title(\"2ï¸âƒ£  Mask Daun (GrabCut)\");    axes[1].axis(\"off\")\n",
                "    axes[2].imshow(cv2.cvtColor(leaf_only, cv2.COLOR_BGR2RGB))\n",
                "    axes[2].set_title(\"3ï¸âƒ£  Daun + BG Hitam â¬›\");     axes[2].axis(\"off\")\n",
                "    axes[3].imshow(rgb_input.astype(\"uint8\"))\n",
                "    axes[3].set_title(\"4ï¸âƒ£  Input Model 224Ã—224\");    axes[3].axis(\"off\")\n",
                "    plt.tight_layout(); plt.show()\n",
                "    img_tensor = tf.expand_dims(tf.constant(rgb_input), axis=0)\n",
                "else:\n",
                "    raw = tf.io.read_file(IMAGE_PATH)\n",
                "    img = tf.image.decode_image(raw, channels=3, expand_animations=False)\n",
                "    img = tf.image.resize(img, IMG_SIZE, method=\"bilinear\")\n",
                "    img_tensor = tf.expand_dims(tf.cast(img, tf.float32), axis=0)\n",
                "\n",
                "probs      = inf_model.predict(img_tensor, verbose=0)[0]\n",
                "pred_idx   = int(np.argmax(probs))\n",
                "pred_label = class_names_inf[pred_idx]\n",
                "confidence = float(probs[pred_idx])\n",
                "\n",
                "fig2, ax2 = plt.subplots(figsize=(9, 4))\n",
                "colors = [\"#27ae60\" if i==pred_idx else \"#3498db\" for i in range(len(class_names_inf))]\n",
                "bars   = ax2.barh(class_names_inf, probs*100, color=colors, edgecolor=\"white\", height=0.6)\n",
                "clr    = \"#27ae60\" if confidence>=0.8 else (\"#e67e22\" if confidence>=0.5 else \"#e74c3c\")\n",
                "ax2.set_title(f\"ğŸ·ï¸  {pred_label.replace('_',' ').title()}  |  {confidence*100:.2f}%\",\n",
                "              fontsize=12, fontweight=\"bold\", color=clr)\n",
                "ax2.set_xlabel(\"Probability (%)\"); ax2.set_xlim(0, 105)\n",
                "for bar, val in zip(bars, probs):\n",
                "    ax2.text(val*100+0.8, bar.get_y()+bar.get_height()/2,\n",
                "             f\"{val*100:.1f}%\", va=\"center\", fontsize=9)\n",
                "ax2.legend(handles=[mpatches.Patch(color=\"#27ae60\", label=\"Predicted\"),\n",
                "                    mpatches.Patch(color=\"#3498db\", label=\"Others\")], loc=\"lower right\")\n",
                "ax2.grid(axis=\"x\", alpha=0.3); plt.tight_layout(); plt.show()\n",
                "print(f\"\\nğŸ¯ Label     : {pred_label}\")\n",
                "print(f\"   Confidence: {confidence*100:.2f}%\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "seg_md_002",
            "metadata": {},
            "source": [
                "### 14c. Batch Prediksi Semua Gambar â†’ CSV (GrabCut Auto-Seed)\n",
                "\n",
                "Proses semua gambar di `test_images/` dengan GrabCut segmentation.\n",
                "Hasil disimpan ke `test_images/prediction_results_grabcut.csv`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "seg_code_003",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 14c) Batch Prediction DENGAN GrabCut â†’ CSV\n",
                "# ----------------------------\n",
                "# âš™ï¸ UBAH DI SINI\n",
                "USE_SEGMENTATION = True\n",
                "# âš™ï¸\n",
                "\n",
                "valid_ext = (\".jpg\",\".jpeg\",\".png\",\".JPG\",\".JPEG\",\".PNG\")\n",
                "all_files = sorted([f for f in os.listdir(CUSTOM_TEST_DIR) if f.endswith(valid_ext)])\n",
                "\n",
                "if not all_files:\n",
                "    print(f\"[WARN] Tidak ada gambar di '{CUSTOM_TEST_DIR}'.\")\n",
                "else:\n",
                "    mode = \"GrabCut\" if USE_SEGMENTATION else \"RAW\"\n",
                "    print(f\"[INFO] {len(all_files)} gambar | Mode: {mode}\")\n",
                "    results = []\n",
                "\n",
                "    for fname in all_files:\n",
                "        fpath = os.path.join(CUSTOM_TEST_DIR, fname)\n",
                "        try:\n",
                "            if USE_SEGMENTATION:\n",
                "                bgr_f = cv2.imread(fpath)\n",
                "                rgb_seg, _, _, _ = preprocess_leaf_grabcut(bgr_f, IMG_SIZE)\n",
                "                t = tf.expand_dims(tf.constant(rgb_seg), axis=0)\n",
                "            else:\n",
                "                raw = tf.io.read_file(fpath)\n",
                "                img = tf.image.decode_image(raw, channels=3, expand_animations=False)\n",
                "                img = tf.image.resize(img, IMG_SIZE, method=\"bilinear\")\n",
                "                t   = tf.expand_dims(tf.cast(img, tf.float32), axis=0)\n",
                "            probs = inf_model.predict(t, verbose=0)[0]\n",
                "            pi    = int(np.argmax(probs))\n",
                "            lbl   = class_names_inf[pi]\n",
                "            conf  = round(float(probs[pi]), 4)\n",
                "            st    = \"OK\"\n",
                "        except Exception as e:\n",
                "            lbl=\"ERROR\"; conf=0.0; st=str(e)\n",
                "        results.append({\"filename\":fname,\"predicted_label\":lbl,\"confidence\":conf,\"status\":st})\n",
                "        print(f\"  [{st}] {fname:30s} â†’ {lbl:20s} ({conf*100:.1f}%)\")\n",
                "\n",
                "    suffix     = \"grabcut\" if USE_SEGMENTATION else \"raw\"\n",
                "    result_df  = pd.DataFrame(results)\n",
                "    out_csv    = os.path.join(CUSTOM_TEST_DIR, f\"prediction_results_{suffix}.csv\")\n",
                "    result_df.to_csv(out_csv, index=False)\n",
                "    print(f\"\\nâœ… Hasil: {out_csv}\")\n",
                "\n",
                "    ok = result_df[result_df.status==\"OK\"]\n",
                "    if not ok.empty:\n",
                "        ok[\"predicted_label\"].value_counts().sort_index().plot(\n",
                "            kind=\"bar\", color=\"#27ae60\", edgecolor=\"white\", figsize=(10,4))\n",
                "        plt.title(f\"Distribusi Prediksi ({mode})\")\n",
                "        plt.xlabel(\"Kelas\"); plt.ylabel(\"Jumlah\")\n",
                "        plt.xticks(rotation=30, ha=\"right\"); plt.grid(axis=\"y\", alpha=0.3)\n",
                "        plt.tight_layout(); plt.show()\n",
                "    print(result_df.to_string(index=False))\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 5,
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}