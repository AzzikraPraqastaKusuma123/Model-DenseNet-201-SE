{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "e1f0b19c",
            "metadata": {},
            "source": [
                "# DenseNet201-SE Standalone (Paddy Disease Classification)\n",
                "\n",
                "Notebook ini adalah implementasi **DenseNet201-SE (Squeeze & Excitation)** murni yang dioptimalkan untuk:\n",
                "1.  **Keras 3 Compatible:** Menggunakan `@register_keras_serializable` untuk layer custom (aman disimpan/diload).\n",
                "2.  **Dynamic Dataset Loading:** Membaca langsung dari folder `train_images`, memastikan **14.112 data augmentasi baru** terbaca sempurna.\n",
                "3.  **Balanced Training:** Memanfaatkan dataset yang sudah seimbang (1.764 gambar/kelas).\n",
                "4.  **2-Stage Training:** Transfer Learning (Freeze Backbone) -> Fine Tuning (Unfreeze Last Layers).\n",
                "\n",
                "**Arsitektur:** DenseNet201 + SE Block + GlobalAveragePooling + Dropout + Dense Softmax."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1b78c299",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import random\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as mpatches\n",
                "import seaborn as sns\n",
                "import tensorflow as tf\n",
                "import keras\n",
                "from keras import layers\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.utils.class_weight import compute_class_weight\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "\n",
                "print(\"TensorFlow:\", tf.__version__)\n",
                "print(\"Keras:\", keras.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b569d9a0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 1) Configuration\n",
                "# ----------------------------\n",
                "DATASET_DIR   = \"paddy-disease-classification\"\n",
                "TRAIN_IMG_DIR = os.path.join(DATASET_DIR, \"train_images\")\n",
                "TEST_IMG_DIR  = os.path.join(DATASET_DIR, \"test_images\")\n",
                "SAMPLE_SUB    = os.path.join(DATASET_DIR, \"sample_submission.csv\")\n",
                "\n",
                "OUTPUT_DIR = \"outputs_densenet201_se_standalone\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "SEED = 42\n",
                "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
                "\n",
                "IMG_SIZE   = (224, 224)\n",
                "BATCH_SIZE = 16\n",
                "AUTOTUNE   = tf.data.AUTOTUNE\n",
                "\n",
                "EPOCHS_STAGE1 = 10\n",
                "EPOCHS_STAGE2 = 15\n",
                "\n",
                "LR1           = 1e-3\n",
                "LR2           = 1e-5\n",
                "UNFREEZE_LAST = 30\n",
                "\n",
                "DROPOUT  = 0.4\n",
                "SE_RATIO = 16\n",
                "\n",
                "USE_FOCAL_LOSS = True\n",
                "GAMMA = 2.0\n",
                "ALPHA = 0.25"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd3a080f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 2) Dynamic Dataset Loading (Folder Scan)\n",
                "# ----------------------------\n",
                "filepaths = []\n",
                "labels    = []\n",
                "\n",
                "classes = sorted(os.listdir(TRAIN_IMG_DIR))\n",
                "classes = [c for c in classes if os.path.isdir(os.path.join(TRAIN_IMG_DIR, c))]\n",
                "print(f\"[INFO] Classes found ({len(classes)}): {classes}\")\n",
                "\n",
                "class_counts = {}\n",
                "for label in classes:\n",
                "    class_dir = os.path.join(TRAIN_IMG_DIR, label)\n",
                "    images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
                "    class_counts[label] = len(images)\n",
                "    for img in images:\n",
                "        filepaths.append(os.path.join(class_dir, img))\n",
                "        labels.append(label)\n",
                "\n",
                "df = pd.DataFrame({'filepath': filepaths, 'label': labels})\n",
                "\n",
                "print(f\"\\n[INFO] Total Dataset: {len(df)} images\")\n",
                "print(\"Distribution per class:\")\n",
                "print(pd.Series(class_counts))\n",
                "\n",
                "class_names  = sorted(df['label'].unique().tolist())\n",
                "num_classes  = len(class_names)\n",
                "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
                "\n",
                "with open(os.path.join(OUTPUT_DIR, \"class_names.json\"), \"w\") as f:\n",
                "    json.dump(class_names, f, indent=2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28e19e30",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 3) Train/Val Split (Stratified 90:10)\n",
                "# ----------------------------\n",
                "train_df, val_df = train_test_split(\n",
                "    df, test_size=0.10, random_state=SEED, stratify=df[\"label\"]\n",
                ")\n",
                "train_df = train_df.reset_index(drop=True)\n",
                "val_df   = val_df.reset_index(drop=True)\n",
                "\n",
                "print(f\"Training Set   : {len(train_df)} images\")\n",
                "print(f\"Validation Set : {len(val_df)} images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d40f37d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 4) tf.data Input Pipeline\n",
                "# ----------------------------\n",
                "def decode_image(path):\n",
                "    img = tf.io.read_file(path)\n",
                "    img = tf.image.decode_jpeg(img, channels=3)\n",
                "    img = tf.image.resize(img, IMG_SIZE, method=\"bilinear\")\n",
                "    return tf.cast(img, tf.float32)\n",
                "\n",
                "def process_path(path, label_idx):\n",
                "    return decode_image(path), tf.one_hot(label_idx, num_classes)\n",
                "\n",
                "def make_ds(dataframe, training=True):\n",
                "    paths         = dataframe[\"filepath\"].values\n",
                "    label_indices = dataframe[\"label\"].map(class_to_idx).values.astype('int32')\n",
                "    ds = tf.data.Dataset.from_tensor_slices((paths, label_indices))\n",
                "    if training:\n",
                "        ds = ds.shuffle(buffer_size=min(len(dataframe), 5000), seed=SEED)\n",
                "    ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
                "    return ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
                "\n",
                "train_ds = make_ds(train_df, training=True)\n",
                "val_ds   = make_ds(val_df, training=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7e8169ce",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 5) Keras 3 Serializable Components\n",
                "# ----------------------------\n",
                "@keras.saving.register_keras_serializable(package=\"custom\")\n",
                "class FocalLoss(tf.keras.losses.Loss):\n",
                "    def __init__(self, gamma=2.0, alpha=0.25, from_logits=False, name=\"focal_loss\"):\n",
                "        super().__init__(name=name)\n",
                "        self.gamma = gamma\n",
                "        self.alpha = alpha\n",
                "        self.from_logits = from_logits\n",
                "\n",
                "    def call(self, y_true, y_pred):\n",
                "        y_true = tf.cast(y_true, tf.float32)\n",
                "        if self.from_logits:\n",
                "            y_pred = tf.nn.softmax(y_pred)\n",
                "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
                "        ce     = -y_true * tf.math.log(y_pred)\n",
                "        weight = self.alpha * tf.pow(1.0 - y_pred, self.gamma)\n",
                "        return tf.reduce_sum(weight * ce, axis=-1)\n",
                "\n",
                "    def get_config(self):\n",
                "        return {\"gamma\": self.gamma, \"alpha\": self.alpha, \"from_logits\": self.from_logits, \"name\": self.name}\n",
                "\n",
                "@keras.saving.register_keras_serializable(package=\"custom\")\n",
                "class DenseNetPreprocess(tf.keras.layers.Layer):\n",
                "    def call(self, x):\n",
                "        return tf.keras.applications.densenet.preprocess_input(x)\n",
                "    def get_config(self):\n",
                "        return {}\n",
                "\n",
                "LOSS_FN = FocalLoss(gamma=GAMMA, alpha=ALPHA) if USE_FOCAL_LOSS else \"categorical_crossentropy\"\n",
                "print(\"Loss Function:\", LOSS_FN.name if hasattr(LOSS_FN, 'name') else LOSS_FN)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fcd69dbd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 6) Model Architecture (DenseNet201 + SE Block)\n",
                "# ----------------------------\n",
                "def se_block(x, ratio=16, name=\"se\"):\n",
                "    c  = int(x.shape[-1])\n",
                "    se = tf.keras.layers.GlobalAveragePooling2D(name=f\"{name}_gap\")(x)\n",
                "    se = tf.keras.layers.Dense(max(1, c // ratio), activation=\"relu\",    name=f\"{name}_fc1\")(se)\n",
                "    se = tf.keras.layers.Dense(c,                  activation=\"sigmoid\", name=f\"{name}_fc2\")(se)\n",
                "    se = tf.keras.layers.Reshape((1, 1, c),                              name=f\"{name}_reshape\")(se)\n",
                "    return tf.keras.layers.Multiply(name=f\"{name}_scale\")([x, se])\n",
                "\n",
                "def build_densenet201_se(num_classes, dropout=DROPOUT, se_ratio=SE_RATIO, name=\"DenseNet201_SE\"):\n",
                "    backbone = tf.keras.applications.DenseNet201(\n",
                "        include_top=False, weights=\"imagenet\",\n",
                "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
                "    )\n",
                "    backbone.trainable = False\n",
                "\n",
                "    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
                "    x   = DenseNetPreprocess(name=\"densenet_preprocess\")(inp)\n",
                "    x   = backbone(x, training=False)\n",
                "    x   = se_block(x, ratio=se_ratio, name=\"se_block\")\n",
                "    x   = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
                "    x   = tf.keras.layers.Dropout(dropout)(x)\n",
                "    out = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
                "    return tf.keras.Model(inp, out, name=name)\n",
                "\n",
                "model = build_densenet201_se(num_classes)\n",
                "model.compile(optimizer=tf.keras.optimizers.Adam(LR1), loss=LOSS_FN, metrics=[\"accuracy\"])\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "da4b1224",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 7) Class Weights\n",
                "# ----------------------------\n",
                "classes_idx  = train_df[\"label\"].map(class_to_idx).values\n",
                "cw           = compute_class_weight(class_weight=\"balanced\", classes=np.unique(classes_idx), y=classes_idx)\n",
                "class_weight = {i: float(w) for i, w in enumerate(cw)}\n",
                "print(\"Class Weights:\", class_weight)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "097c14ae",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 8) Train & Eval Plot Helper\n",
                "# ----------------------------\n",
                "def plot_history(history, stage_name=\"Training\"):\n",
                "    acc      = history.history['accuracy']\n",
                "    val_acc  = history.history['val_accuracy']\n",
                "    loss     = history.history['loss']\n",
                "    val_loss = history.history['val_loss']\n",
                "    epochs   = range(1, len(acc) + 1)\n",
                "\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
                "    fig.suptitle(f\"Train Evaluation ‚Äî {stage_name}\", fontsize=14, fontweight='bold')\n",
                "\n",
                "    ax1.plot(epochs, acc,     'bo-', label='Train Accuracy')\n",
                "    ax1.plot(epochs, val_acc, 'ro-', label='Val Accuracy')\n",
                "    ax1.set_title('Accuracy'); ax1.set_xlabel('Epoch'); ax1.set_ylabel('Accuracy')\n",
                "    ax1.legend(); ax1.grid(True, alpha=0.3)\n",
                "\n",
                "    ax2.plot(epochs, loss,     'bo-', label='Train Loss')\n",
                "    ax2.plot(epochs, val_loss, 'ro-', label='Val Loss')\n",
                "    ax2.set_title('Loss'); ax2.set_xlabel('Epoch'); ax2.set_ylabel('Loss')\n",
                "    ax2.legend(); ax2.grid(True, alpha=0.3)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c532eaaf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 9) STAGE 1: Train Head Only\n",
                "# ----------------------------\n",
                "checkpoint_path = os.path.join(OUTPUT_DIR, \"best_stage1.keras\")\n",
                "\n",
                "callbacks_stage1 = [\n",
                "    tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
                "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1)\n",
                "]\n",
                "\n",
                "print(\"\\nüöÄ Starting Stage 1 Training (Head Only)...\")\n",
                "history1 = model.fit(\n",
                "    train_ds, validation_data=val_ds,\n",
                "    epochs=EPOCHS_STAGE1, callbacks=callbacks_stage1, class_weight=class_weight\n",
                ")\n",
                "plot_history(history1, \"Stage 1 ‚Äî Head Training\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43e920b7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 10) STAGE 2: Fine Tuning\n",
                "# ----------------------------\n",
                "model = tf.keras.models.load_model(\n",
                "    checkpoint_path,\n",
                "    custom_objects={\"FocalLoss\": FocalLoss, \"DenseNetPreprocess\": DenseNetPreprocess}\n",
                ")\n",
                "\n",
                "backbone = model.layers[2]\n",
                "backbone.trainable = True\n",
                "\n",
                "for layer in backbone.layers[:-UNFREEZE_LAST]:\n",
                "    layer.trainable = False\n",
                "for layer in backbone.layers:\n",
                "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
                "        layer.trainable = False\n",
                "\n",
                "model.compile(optimizer=tf.keras.optimizers.Adam(LR2), loss=LOSS_FN, metrics=[\"accuracy\"])\n",
                "\n",
                "final_checkpoint_path = os.path.join(OUTPUT_DIR, \"densenet201_se_final.keras\")\n",
                "\n",
                "callbacks_stage2 = [\n",
                "    tf.keras.callbacks.ModelCheckpoint(final_checkpoint_path, monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
                "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True, verbose=1),\n",
                "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1)\n",
                "]\n",
                "\n",
                "print(f\"\\nüöÄ Starting Stage 2 Fine-Tuning (Unfreezing last {UNFREEZE_LAST} layers)...\")\n",
                "history2 = model.fit(\n",
                "    train_ds, validation_data=val_ds,\n",
                "    epochs=EPOCHS_STAGE2, callbacks=callbacks_stage2, class_weight=class_weight\n",
                ")\n",
                "plot_history(history2, \"Stage 2 ‚Äî Fine Tuning\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "351192f9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 11) Evaluasi: Confusion Matrix & Report\n",
                "# ----------------------------\n",
                "best_model = tf.keras.models.load_model(\n",
                "    final_checkpoint_path,\n",
                "    custom_objects={\"FocalLoss\": FocalLoss, \"DenseNetPreprocess\": DenseNetPreprocess}\n",
                ")\n",
                "\n",
                "print(\"\\nüìä Generating Evaluation Metrics...\")\n",
                "\n",
                "y_pred, y_true = [], []\n",
                "for bx, by in val_ds:\n",
                "    probs = best_model.predict(bx, verbose=0)\n",
                "    y_pred.extend(np.argmax(probs, axis=1))\n",
                "    y_true.extend(np.argmax(by.numpy(), axis=1))\n",
                "\n",
                "print(\"\\n‚úÖ Classification Report:\")\n",
                "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
                "\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "plt.figure(figsize=(11, 8))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
                "plt.xlabel('Predicted'); plt.ylabel('Ground Truth')\n",
                "plt.title('Confusion Matrix ‚Äî Validation Set')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4e83b055",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 12) Save Model\n",
                "# ----------------------------\n",
                "print(\"\\nüíæ Model saved successfully at:\")\n",
                "print(f\"   - {final_checkpoint_path}\")\n",
                "print(\"   (Gunakan file ini untuk inference atau deployment)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "inf_md_001",
            "metadata": {},
            "source": [
                "---\n",
                "## üîç 13) Inference Helper\n",
                "\n",
                "Bagian ini digunakan untuk **menguji model** pada gambar baru setelah proses training selesai.\n",
                "**Tidak perlu training ulang** ‚Äî cukup load model yang sudah tersimpan dan jalankan prediksi.\n",
                "\n",
                "### Cara menjalankan:\n",
                "\n",
                "| Urutan | Cell | Fungsi |\n",
                "|:---:|---|---|\n",
                "| **1** | **Cell 13a** | Load semua library + model + class names |\n",
                "| **2** | **Cell 13b** | Prediksi satu gambar dari folder `test_images/` |\n",
                "| **3** | **Cell 13c** | Prediksi semua gambar di folder `test_images/` ‚Üí simpan ke CSV |\n",
                "\n",
                "> ‚ö†Ô∏è **Cell 13a harus dijalankan pertama.** Letakkan gambar uji di folder `test_images/`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "inf_code_001",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 13a) Load Model + Library untuk Inference\n",
                "# WAJIB dijalankan pertama sebelum Cell 13b / 13c / 14b / 14c\n",
                "# ----------------------------\n",
                "import os\n",
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as mpatches\n",
                "import tensorflow as tf\n",
                "import keras\n",
                "\n",
                "# ============================================================\n",
                "# AUTO-DETECT path model ‚Äî bekerja di CWD manapun\n",
                "# ============================================================\n",
                "def _find_model_path():\n",
                "    \"\"\"Cari file model .keras secara otomatis dari beberapa lokasi kandidat.\"\"\"\n",
                "    # Folder project (absolut, diambil dari lokasi notebook ini)\n",
                "    base_candidates = [\n",
                "        os.getcwd(),\n",
                "        r\"c:\\Users\\azzik\\Documents\\Tugas Akhir\\Skripsi\\Model DenseNet-201\",\n",
                "    ]\n",
                "    model_subpaths = [\n",
                "        os.path.join(\"model\", \"outputs_densenet201_se_standalone\", \"densenet201_se_final.keras\"),\n",
                "        os.path.join(\"outputs_densenet201_se_standalone\", \"densenet201_se_final.keras\"),\n",
                "    ]\n",
                "    for base in base_candidates:\n",
                "        for sub in model_subpaths:\n",
                "            candidate = os.path.join(base, sub)\n",
                "            if os.path.exists(candidate):\n",
                "                return candidate\n",
                "    return None\n",
                "\n",
                "def _find_class_names_path():\n",
                "    \"\"\"Cari file class_names.json secara otomatis.\"\"\"\n",
                "    base_candidates = [\n",
                "        os.getcwd(),\n",
                "        r\"c:\\Users\\azzik\\Documents\\Tugas Akhir\\Skripsi\\Model DenseNet-201\",\n",
                "    ]\n",
                "    cn_subpaths = [\n",
                "        os.path.join(\"model\", \"outputs_densenet201_se_standalone\", \"class_names.json\"),\n",
                "        os.path.join(\"outputs_densenet201_se_standalone\", \"class_names.json\"),\n",
                "        \"class_names.json\",\n",
                "    ]\n",
                "    for base in base_candidates:\n",
                "        for sub in cn_subpaths:\n",
                "            candidate = os.path.join(base, sub)\n",
                "            if os.path.exists(candidate):\n",
                "                return candidate\n",
                "    return None\n",
                "\n",
                "# Resolusi path\n",
                "print(f\"[INFO] Working Directory : {os.getcwd()}\")\n",
                "\n",
                "MODEL_PATH       = _find_model_path()\n",
                "CLASS_NAMES_PATH = _find_class_names_path()\n",
                "CUSTOM_TEST_DIR  = \"test_images\"   # Folder gambar uji (di LUAR paddy-disease-classification)\n",
                "IMG_SIZE         = (224, 224)\n",
                "\n",
                "if MODEL_PATH is None:\n",
                "    raise FileNotFoundError(\n",
                "        \"‚ùå File model tidak ditemukan!\\n\"\n",
                "        \"   Pastikan file 'densenet201_se_final.keras' ada di:\\n\"\n",
                "        \"   model/outputs_densenet201_se_standalone/densenet201_se_final.keras\"\n",
                "    )\n",
                "if CLASS_NAMES_PATH is None:\n",
                "    raise FileNotFoundError(\n",
                "        \"‚ùå File class_names.json tidak ditemukan!\\n\"\n",
                "        \"   Pastikan file ada di folder output yang sama dengan model.\"\n",
                "    )\n",
                "\n",
                "print(f\"[INFO] Model ditemukan  : {MODEL_PATH}\")\n",
                "print(f\"[INFO] Classes path     : {CLASS_NAMES_PATH}\")\n",
                "\n",
                "# Re-register custom components\n",
                "@keras.saving.register_keras_serializable(package=\"custom\")\n",
                "class FocalLoss(tf.keras.losses.Loss):\n",
                "    def __init__(self, gamma=2.0, alpha=0.25, from_logits=False, name=\"focal_loss\"):\n",
                "        super().__init__(name=name)\n",
                "        self.gamma = gamma; self.alpha = alpha; self.from_logits = from_logits\n",
                "    def call(self, y_true, y_pred):\n",
                "        if self.from_logits: y_pred = tf.nn.softmax(y_pred)\n",
                "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
                "        ce = -tf.cast(y_true, tf.float32) * tf.math.log(y_pred)\n",
                "        return tf.reduce_sum(self.alpha * tf.pow(1.0 - y_pred, self.gamma) * ce, axis=-1)\n",
                "    def get_config(self):\n",
                "        return {\"gamma\": self.gamma, \"alpha\": self.alpha, \"from_logits\": self.from_logits, \"name\": self.name}\n",
                "\n",
                "@keras.saving.register_keras_serializable(package=\"custom\")\n",
                "class DenseNetPreprocess(tf.keras.layers.Layer):\n",
                "    def call(self, x): return tf.keras.applications.densenet.preprocess_input(x)\n",
                "    def get_config(self): return {}\n",
                "\n",
                "CUSTOM_OBJ = {\"FocalLoss\": FocalLoss, \"DenseNetPreprocess\": DenseNetPreprocess}\n",
                "\n",
                "# Load model\n",
                "print(f\"\\n[INFO] Loading model...\")\n",
                "inf_model = tf.keras.models.load_model(MODEL_PATH, custom_objects=CUSTOM_OBJ)\n",
                "print(\"[INFO] ‚úÖ Model berhasil di-load!\")\n",
                "\n",
                "# Load class names\n",
                "with open(CLASS_NAMES_PATH, \"r\") as f:\n",
                "    class_names_inf = json.load(f)\n",
                "\n",
                "print(f\"[INFO] Classes ({len(class_names_inf)}): {class_names_inf}\")\n",
                "print(f\"[INFO] Test Images Dir  : '{CUSTOM_TEST_DIR}'\")\n",
                "print(\"\\n‚úÖ Siap! Jalankan Cell 13b, 13c, 14b, atau 14c.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "inf_md_002",
            "metadata": {},
            "source": [
                "### 13b. Prediksi Satu Gambar (Tanpa Segmentasi)\n",
                "\n",
                "Ubah `IMAGE_FILE` ke nama gambar di folder `test_images/`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "inf_code_002",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 13b) Prediksi SATU Gambar (Tanpa Segmentasi)\n",
                "# ----------------------------\n",
                "def predict_single_image(model, class_names, image_path, img_size=(224, 224)):\n",
                "    import matplotlib.patches as mpatches\n",
                "    if not os.path.exists(image_path):\n",
                "        raise FileNotFoundError(f\"‚ùå Gambar tidak ditemukan: '{image_path}'\")\n",
                "\n",
                "    img_raw     = tf.io.read_file(image_path)\n",
                "    img         = tf.image.decode_image(img_raw, channels=3, expand_animations=False)\n",
                "    img_display = img.numpy().astype('uint8')\n",
                "    img         = tf.image.resize(img, img_size, method=\"bilinear\")\n",
                "    img         = tf.cast(img, tf.float32)\n",
                "    img         = tf.expand_dims(img, axis=0)\n",
                "\n",
                "    probs      = model.predict(img, verbose=0)[0]\n",
                "    pred_idx   = int(np.argmax(probs))\n",
                "    pred_label = class_names[pred_idx]\n",
                "    confidence = float(probs[pred_idx])\n",
                "\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    fig.suptitle(\"Prediksi DenseNet201-SE (Tanpa Segmentasi)\", fontsize=13, fontweight='bold')\n",
                "    ax1.imshow(img_display); ax1.axis('off')\n",
                "    color = '#27ae60' if confidence >= 0.80 else ('#e67e22' if confidence >= 0.50 else '#e74c3c')\n",
                "    ax1.set_title(f\"üè∑Ô∏è  {pred_label.replace('_',' ').title()}\\nConfidence: {confidence*100:.2f}%\",\n",
                "                  fontsize=12, fontweight='bold', color=color)\n",
                "    colors = ['#27ae60' if i == pred_idx else '#3498db' for i in range(len(class_names))]\n",
                "    bars   = ax2.barh(class_names, probs * 100, color=colors, edgecolor='white', height=0.6)\n",
                "    ax2.set_xlabel('Probability (%)'); ax2.set_title('Distribusi Probabilitas')\n",
                "    ax2.set_xlim(0, 105)\n",
                "    for bar, val in zip(bars, probs):\n",
                "        ax2.text(val * 100 + 0.8, bar.get_y() + bar.get_height() / 2,\n",
                "                 f'{val*100:.1f}%', va='center', fontsize=9)\n",
                "    ax2.legend(handles=[mpatches.Patch(color='#27ae60', label='Predicted'),\n",
                "                        mpatches.Patch(color='#3498db', label='Others')], loc='lower right')\n",
                "    ax2.grid(axis='x', alpha=0.3)\n",
                "    plt.tight_layout(); plt.show()\n",
                "\n",
                "    print(f\"\\nüéØ Kelas: {pred_label} | Confidence: {confidence*100:.2f}%\")\n",
                "    return pred_label, confidence, probs\n",
                "\n",
                "IMAGE_FILE = \"image.png\"   # ‚Üê Ganti nama file\n",
                "IMAGE_PATH = os.path.join(CUSTOM_TEST_DIR, IMAGE_FILE)\n",
                "pred_label, confidence, all_probs = predict_single_image(inf_model, class_names_inf, IMAGE_PATH)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "inf_md_003",
            "metadata": {},
            "source": [
                "### 13c. Prediksi Batch `test_images/` ‚Üí CSV (Tanpa Segmentasi)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "inf_code_003",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 13c) Batch Prediction ‚Üí CSV (Tanpa Segmentasi)\n",
                "# ----------------------------\n",
                "def predict_all_in_folder(model, class_names, test_dir, img_size=(224, 224), batch_size=16):\n",
                "    valid_ext = ('.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG')\n",
                "    all_files = sorted([f for f in os.listdir(test_dir) if f.endswith(valid_ext)])\n",
                "    if not all_files:\n",
                "        print(f\"[WARN] Tidak ada gambar di '{test_dir}'.\"); return None\n",
                "    print(f\"[INFO] Ditemukan {len(all_files)} gambar\")\n",
                "    test_paths = [os.path.join(test_dir, f) for f in all_files]\n",
                "\n",
                "    def load_img(path):\n",
                "        raw = tf.io.read_file(path)\n",
                "        img = tf.image.decode_image(raw, channels=3, expand_animations=False)\n",
                "        img = tf.image.resize(img, img_size, method=\"bilinear\")\n",
                "        return tf.cast(img, tf.float32)\n",
                "\n",
                "    test_ds = (tf.data.Dataset.from_tensor_slices(test_paths)\n",
                "               .map(load_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
                "               .batch(batch_size).prefetch(tf.data.AUTOTUNE))\n",
                "\n",
                "    print(\"[INFO] Running inference...\")\n",
                "    all_probs   = np.concatenate([model.predict(b, verbose=0) for b in test_ds], axis=0)\n",
                "    pred_idx    = np.argmax(all_probs, axis=1)\n",
                "    pred_labels = [class_names[i] for i in pred_idx]\n",
                "    confs       = [round(float(all_probs[i, pred_idx[i]]), 4) for i in range(len(pred_idx))]\n",
                "\n",
                "    result_df = pd.DataFrame({\"filename\": all_files, \"predicted_label\": pred_labels, \"confidence\": confs})\n",
                "    out_csv   = os.path.join(test_dir, \"prediction_results.csv\")\n",
                "    result_df.to_csv(out_csv, index=False)\n",
                "    print(f\"[INFO] ‚úÖ Hasil disimpan: {out_csv}\")\n",
                "\n",
                "    pd.Series(pred_labels).value_counts().sort_index().plot(\n",
                "        kind='bar', color='#3498db', edgecolor='white', figsize=(10, 4))\n",
                "    plt.title('Distribusi Prediksi'); plt.xlabel('Kelas'); plt.ylabel('Jumlah')\n",
                "    plt.xticks(rotation=30, ha='right'); plt.grid(axis='y', alpha=0.3)\n",
                "    plt.tight_layout(); plt.show()\n",
                "    print(result_df.to_string(index=False))\n",
                "    return result_df\n",
                "\n",
                "result_df = predict_all_in_folder(inf_model, class_names_inf, CUSTOM_TEST_DIR)\n",
                "if result_df is not None: result_df.head(20)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "seg_md_001",
            "metadata": {},
            "source": [
                "---\n",
                "## üçÉ 14) Leaf Segmentation + Inference (Upgrade Akurasi)\n",
                "\n",
                "**Apa itu Leaf Segmentation?**\n",
                "Proses memisahkan objek daun dari background sebelum gambar masuk ke model.\n",
                "Model DenseNet201-SE jadi fokus ke:\n",
                "- ‚úÖ Tekstur bercak/lesi penyakit\n",
                "- ‚úÖ Warna daun & pola area terinfeksi\n",
                "- ‚ùå Tidak terganggu tanah, tangan, pot, langit, meja\n",
                "\n",
                "**Teknik:** HSV Color Masking + Morphology + Crop ROI\n",
                "\n",
                "```\n",
                "Gambar Raw ‚Üí leaf_mask_hsv() ‚Üí crop_to_leaf() ‚Üí resize(224,224) ‚Üí Model\n",
                "```\n",
                "\n",
                "| Cell | Fungsi |\n",
                "|:---:|---|\n",
                "| **14a** | Definisi fungsi segmentasi (wajib dijalankan dulu) |\n",
                "| **14b** | Prediksi 1 gambar dengan segmentasi + visualisasi sebelum/sesudah |\n",
                "| **14c** | Batch prediction dengan segmentasi ‚Üí CSV |\n",
                "\n",
                "> ‚ö†Ô∏è **Prasyarat:** Cell **13a** harus sudah dijalankan. Install opencv jika belum: `pip install opencv-python`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "seg_code_001",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 14a) Definisi Fungsi Leaf Segmentation\n",
                "# Wajib dijalankan sebelum Cell 14b dan 14c\n",
                "# ----------------------------\n",
                "try:\n",
                "    import cv2\n",
                "    print(f\"[INFO] ‚úÖ OpenCV version: {cv2.__version__}\")\n",
                "except ImportError:\n",
                "    raise ImportError(\n",
                "        \"OpenCV belum terinstall.\\n\"\n",
                "        \"Jalankan di terminal: pip install opencv-python\\n\"\n",
                "        \"Atau di notebook cell baru: !pip install opencv-python\"\n",
                "    )\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "def leaf_mask_hsv(bgr_img):\n",
                "    \"\"\"\n",
                "    Membuat mask biner daun dari gambar BGR menggunakan HSV.\n",
                "    Return: mask biner (uint8) 0=background, 255=daun\n",
                "    \"\"\"\n",
                "    hsv    = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2HSV)\n",
                "    lower  = np.array([25,  40,  40],  dtype=np.uint8)  # Hijau muda\n",
                "    upper  = np.array([95, 255, 255],  dtype=np.uint8)  # Hijau tua\n",
                "    mask   = cv2.inRange(hsv, lower, upper)\n",
                "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
                "    mask   = cv2.morphologyEx(mask, cv2.MORPH_OPEN,  kernel, iterations=1)\n",
                "    mask   = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
                "    return mask\n",
                "\n",
                "def crop_to_leaf(bgr_img, mask, pad=10):\n",
                "    \"\"\"\n",
                "    Crop ke bounding box kontur terbesar. Fallback ke gambar asli jika gagal.\n",
                "    \"\"\"\n",
                "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
                "    if not contours:\n",
                "        return bgr_img\n",
                "    c        = max(contours, key=cv2.contourArea)\n",
                "    x, y, w, h = cv2.boundingRect(c)\n",
                "    H, W     = bgr_img.shape[:2]\n",
                "    x1, y1   = max(0, x - pad), max(0, y - pad)\n",
                "    x2, y2   = min(W, x + w + pad), min(H, y + h + pad)\n",
                "    return bgr_img[y1:y2, x1:x2]\n",
                "\n",
                "def preprocess_leaf_for_model(image_path, target_size=(224, 224)):\n",
                "    \"\"\"\n",
                "    Pipeline: imread ‚Üí HSV mask ‚Üí crop ROI ‚Üí resize ‚Üí BGR‚ÜíRGB ‚Üí float32\n",
                "    Return: (rgb_array, mask, bgr_original)\n",
                "    \"\"\"\n",
                "    bgr = cv2.imread(image_path)\n",
                "    if bgr is None:\n",
                "        raise FileNotFoundError(f\"Gambar tidak ditemukan: {image_path}\")\n",
                "    mask    = leaf_mask_hsv(bgr)\n",
                "    cropped = crop_to_leaf(bgr, mask, pad=10)\n",
                "    resized = cv2.resize(cropped, target_size, interpolation=cv2.INTER_LINEAR)\n",
                "    rgb     = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
                "    return rgb.astype(np.float32), mask, bgr\n",
                "\n",
                "print(\"[INFO] ‚úÖ Fungsi Leaf Segmentation siap.\")\n",
                "print(\"       ‚Üí Cell 14b: prediksi 1 gambar dengan visualisasi segmentasi\")\n",
                "print(\"       ‚Üí Cell 14c: batch prediction seluruh test_images/ ‚Üí CSV\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "seg_md_002",
            "metadata": {},
            "source": [
                "### 14b. Prediksi 1 Gambar DENGAN Segmentasi\n",
                "\n",
                "Menampilkan **3 panel**:\n",
                "1. Gambar asli\n",
                "2. Mask HSV (area daun = putih)\n",
                "3. Gambar sesudah segmentasi (yang masuk ke model)\n",
                "\n",
                "Ubah `IMAGE_FILE` ke nama gambar di folder `test_images/`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "seg_code_002",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 14b) Prediksi 1 Gambar DENGAN Leaf Segmentation\n",
                "# ----------------------------\n",
                "def predict_with_segmentation(model, class_names, image_path, img_size=(224, 224)):\n",
                "    import matplotlib.patches as mpatches\n",
                "\n",
                "    if not os.path.exists(image_path):\n",
                "        raise FileNotFoundError(f\"‚ùå File tidak ditemukan: '{image_path}'\")\n",
                "\n",
                "    # Segmentasi\n",
                "    rgb_segmented, mask, bgr_original = preprocess_leaf_for_model(image_path, target_size=img_size)\n",
                "\n",
                "    # Panel visualisasi: Asli | Mask | Hasil Segmentasi\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
                "    fig.suptitle(\"Proses Leaf Segmentation\", fontsize=13, fontweight='bold')\n",
                "    axes[0].imshow(cv2.cvtColor(bgr_original, cv2.COLOR_BGR2RGB))\n",
                "    axes[0].set_title(\"1Ô∏è‚É£  Gambar Asli\", fontsize=11); axes[0].axis('off')\n",
                "    axes[1].imshow(mask, cmap='gray')\n",
                "    axes[1].set_title(\"2Ô∏è‚É£  HSV Mask (Daun=Putih)\", fontsize=11); axes[1].axis('off')\n",
                "    axes[2].imshow(rgb_segmented.astype('uint8'))\n",
                "    axes[2].set_title(\"3Ô∏è‚É£  Input Model (Setelah Segmentasi)\", fontsize=11); axes[2].axis('off')\n",
                "    plt.tight_layout(); plt.show()\n",
                "\n",
                "    # Prediksi\n",
                "    img_tensor = tf.expand_dims(tf.constant(rgb_segmented), axis=0)\n",
                "    probs      = model.predict(img_tensor, verbose=0)[0]\n",
                "    pred_idx   = int(np.argmax(probs))\n",
                "    pred_label = class_names[pred_idx]\n",
                "    confidence = float(probs[pred_idx])\n",
                "\n",
                "    # Bar chart hasil\n",
                "    fig2, ax = plt.subplots(figsize=(9, 4))\n",
                "    colors   = ['#27ae60' if i == pred_idx else '#3498db' for i in range(len(class_names))]\n",
                "    bars     = ax.barh(class_names, probs * 100, color=colors, edgecolor='white', height=0.6)\n",
                "    ax.set_xlabel('Probability (%)'); ax.set_xlim(0, 105)\n",
                "    ax.set_title(f'Hasil Prediksi (Dengan Segmentasi) ‚Äî {pred_label.title()} ({confidence*100:.1f}%)')\n",
                "    for bar, val in zip(bars, probs):\n",
                "        ax.text(val * 100 + 0.8, bar.get_y() + bar.get_height() / 2,\n",
                "                f'{val*100:.1f}%', va='center', fontsize=9)\n",
                "    ax.legend(handles=[mpatches.Patch(color='#27ae60', label='Predicted'),\n",
                "                       mpatches.Patch(color='#3498db', label='Others')], loc='lower right')\n",
                "    ax.grid(axis='x', alpha=0.3)\n",
                "    plt.tight_layout(); plt.show()\n",
                "\n",
                "    print(f\"\\nüéØ Kelas: {pred_label} | Confidence: {confidence*100:.2f}%\")\n",
                "    return pred_label, confidence, probs\n",
                "\n",
                "\n",
                "# ===== GANTI nama file di sini =====\n",
                "IMAGE_FILE = \"image.png\"   # ‚Üê nama gambar di folder test_images/\n",
                "# ====================================\n",
                "\n",
                "IMAGE_PATH = os.path.join(CUSTOM_TEST_DIR, IMAGE_FILE)\n",
                "print(f\"[INFO] Memproses: {IMAGE_PATH}\")\n",
                "pred_seg, conf_seg, probs_seg = predict_with_segmentation(inf_model, class_names_inf, IMAGE_PATH)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "seg_md_003",
            "metadata": {},
            "source": [
                "### 14c. Batch Prediction DENGAN Segmentasi ‚Üí CSV\n",
                "\n",
                "Semua gambar di `test_images/` diproses dengan segmentasi ‚Üí disimpan ke `prediction_results_segmented.csv`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "seg_code_003",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 14c) Batch Prediction DENGAN Segmentasi ‚Üí CSV\n",
                "# ----------------------------\n",
                "def predict_batch_with_segmentation(model, class_names, test_dir, img_size=(224, 224)):\n",
                "    valid_ext = ('.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG')\n",
                "    all_files = sorted([f for f in os.listdir(test_dir) if f.endswith(valid_ext)])\n",
                "\n",
                "    if not all_files:\n",
                "        print(f\"[WARN] Tidak ada gambar di '{test_dir}'.\"); return None\n",
                "\n",
                "    print(f\"[INFO] {len(all_files)} gambar ditemukan. Memproses dengan segmentasi...\")\n",
                "    results = []\n",
                "\n",
                "    for fname in all_files:\n",
                "        fpath = os.path.join(test_dir, fname)\n",
                "        try:\n",
                "            rgb_seg, _, _ = preprocess_leaf_for_model(fpath, target_size=img_size)\n",
                "            img_tensor    = tf.expand_dims(tf.constant(rgb_seg), axis=0)\n",
                "            probs         = model.predict(img_tensor, verbose=0)[0]\n",
                "            pred_idx      = int(np.argmax(probs))\n",
                "            pred_label    = class_names[pred_idx]\n",
                "            confidence    = round(float(probs[pred_idx]), 4)\n",
                "            status        = \"OK\"\n",
                "        except Exception as e:\n",
                "            pred_label = \"ERROR\"; confidence = 0.0; status = str(e)\n",
                "\n",
                "        results.append({\"filename\": fname, \"predicted_label\": pred_label,\n",
                "                        \"confidence\": confidence, \"status\": status})\n",
                "        print(f\"  [{status}] {fname:30s} ‚Üí {pred_label:20s} ({confidence*100:.1f}%)\")\n",
                "\n",
                "    result_df = pd.DataFrame(results)\n",
                "    out_csv   = os.path.join(test_dir, \"prediction_results_segmented.csv\")\n",
                "    result_df.to_csv(out_csv, index=False)\n",
                "    print(f\"\\n[INFO] ‚úÖ Hasil disimpan: {out_csv}\")\n",
                "\n",
                "    ok_df = result_df[result_df['status'] == 'OK']\n",
                "    if not ok_df.empty:\n",
                "        ok_df['predicted_label'].value_counts().sort_index().plot(\n",
                "            kind='bar', color='#27ae60', edgecolor='white', figsize=(10, 4))\n",
                "        plt.title('Distribusi Prediksi (Dengan Segmentasi)')\n",
                "        plt.xlabel('Kelas'); plt.ylabel('Jumlah')\n",
                "        plt.xticks(rotation=30, ha='right'); plt.grid(axis='y', alpha=0.3)\n",
                "        plt.tight_layout(); plt.show()\n",
                "\n",
                "    return result_df\n",
                "\n",
                "\n",
                "seg_results = predict_batch_with_segmentation(inf_model, class_names_inf, CUSTOM_TEST_DIR)\n",
                "if seg_results is not None:\n",
                "    seg_results.head(20)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 5,
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}