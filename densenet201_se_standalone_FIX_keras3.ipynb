{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "e1f0b19c",
            "metadata": {},
            "source": [
                "# DenseNet201-SE Standalone (Paddy Disease Classification)\n",
                "\n",
                "Notebook ini adalah implementasi **DenseNet201-SE (Squeeze & Excitation)** murni yang dioptimalkan untuk:\n",
                "1.  **Keras 3 Compatible:** Menggunakan `@register_keras_serializable` untuk layer custom (aman disimpan/diload).\n",
                "2.  **Dynamic Dataset Loading:** Membaca langsung dari folder `train_images`, memastikan **14.112 data augmentasi baru** terbaca sempurna.\n",
                "3.  **Balanced Training:** Memanfaatkan dataset yang sudah seimbang (1.764 gambar/kelas).\n",
                "4.  **2-Stage Training:** Transfer Learning (Freeze Backbone) -> Fine Tuning (Unfreeze Last Layers).\n",
                "\n",
                "**Arsitektur:** DenseNet201 + SE Block + GlobalAveragePooling + Dropout + Dense Softmax."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1b78c299",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import random\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as mpatches\n",
                "import seaborn as sns\n",
                "import tensorflow as tf\n",
                "import keras\n",
                "from keras import layers\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.utils.class_weight import compute_class_weight\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "\n",
                "print(\"TensorFlow:\", tf.__version__)\n",
                "print(\"Keras:\", keras.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b569d9a0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 1) Configuration\n",
                "# ----------------------------\n",
                "DATASET_DIR   = \"paddy-disease-classification\"\n",
                "TRAIN_IMG_DIR = os.path.join(DATASET_DIR, \"train_images\")\n",
                "TEST_IMG_DIR  = os.path.join(DATASET_DIR, \"test_images\")\n",
                "SAMPLE_SUB    = os.path.join(DATASET_DIR, \"sample_submission.csv\")\n",
                "\n",
                "OUTPUT_DIR = \"outputs_densenet201_se_standalone\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "SEED = 42\n",
                "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
                "\n",
                "IMG_SIZE   = (224, 224)\n",
                "BATCH_SIZE = 16\n",
                "AUTOTUNE   = tf.data.AUTOTUNE\n",
                "\n",
                "EPOCHS_STAGE1 = 10\n",
                "EPOCHS_STAGE2 = 15\n",
                "\n",
                "LR1           = 1e-3\n",
                "LR2           = 1e-5\n",
                "UNFREEZE_LAST = 30\n",
                "\n",
                "DROPOUT  = 0.4\n",
                "SE_RATIO = 16\n",
                "\n",
                "USE_FOCAL_LOSS = True\n",
                "GAMMA = 2.0\n",
                "ALPHA = 0.25"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd3a080f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 2) Dynamic Dataset Loading (Folder Scan)\n",
                "# ----------------------------\n",
                "filepaths = []\n",
                "labels    = []\n",
                "\n",
                "classes = sorted(os.listdir(TRAIN_IMG_DIR))\n",
                "classes = [c for c in classes if os.path.isdir(os.path.join(TRAIN_IMG_DIR, c))]\n",
                "print(f\"[INFO] Classes found ({len(classes)}): {classes}\")\n",
                "\n",
                "class_counts = {}\n",
                "for label in classes:\n",
                "    class_dir = os.path.join(TRAIN_IMG_DIR, label)\n",
                "    images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
                "    class_counts[label] = len(images)\n",
                "    for img in images:\n",
                "        filepaths.append(os.path.join(class_dir, img))\n",
                "        labels.append(label)\n",
                "\n",
                "df = pd.DataFrame({'filepath': filepaths, 'label': labels})\n",
                "\n",
                "print(f\"\\n[INFO] Total Dataset: {len(df)} images\")\n",
                "print(\"Distribution per class:\")\n",
                "print(pd.Series(class_counts))\n",
                "\n",
                "class_names  = sorted(df['label'].unique().tolist())\n",
                "num_classes  = len(class_names)\n",
                "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
                "\n",
                "with open(os.path.join(OUTPUT_DIR, \"class_names.json\"), \"w\") as f:\n",
                "    json.dump(class_names, f, indent=2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28e19e30",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 3) Train/Val Split (Stratified 90:10)\n",
                "# ----------------------------\n",
                "train_df, val_df = train_test_split(\n",
                "    df, test_size=0.10, random_state=SEED, stratify=df[\"label\"]\n",
                ")\n",
                "train_df = train_df.reset_index(drop=True)\n",
                "val_df   = val_df.reset_index(drop=True)\n",
                "\n",
                "print(f\"Training Set   : {len(train_df)} images\")\n",
                "print(f\"Validation Set : {len(val_df)} images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d40f37d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 4) tf.data Input Pipeline\n",
                "# ----------------------------\n",
                "def decode_image(path):\n",
                "    img = tf.io.read_file(path)\n",
                "    img = tf.image.decode_jpeg(img, channels=3)\n",
                "    img = tf.image.resize(img, IMG_SIZE, method=\"bilinear\")\n",
                "    return tf.cast(img, tf.float32)\n",
                "\n",
                "def process_path(path, label_idx):\n",
                "    return decode_image(path), tf.one_hot(label_idx, num_classes)\n",
                "\n",
                "def make_ds(dataframe, training=True):\n",
                "    paths         = dataframe[\"filepath\"].values\n",
                "    label_indices = dataframe[\"label\"].map(class_to_idx).values.astype('int32')\n",
                "    ds = tf.data.Dataset.from_tensor_slices((paths, label_indices))\n",
                "    if training:\n",
                "        ds = ds.shuffle(buffer_size=min(len(dataframe), 5000), seed=SEED)\n",
                "    ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
                "    return ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
                "\n",
                "train_ds = make_ds(train_df, training=True)\n",
                "val_ds   = make_ds(val_df, training=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7e8169ce",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 5) Keras 3 Serializable Components\n",
                "# ----------------------------\n",
                "@keras.saving.register_keras_serializable(package=\"custom\")\n",
                "class FocalLoss(tf.keras.losses.Loss):\n",
                "    def __init__(self, gamma=2.0, alpha=0.25, from_logits=False, name=\"focal_loss\"):\n",
                "        super().__init__(name=name)\n",
                "        self.gamma = gamma\n",
                "        self.alpha = alpha\n",
                "        self.from_logits = from_logits\n",
                "\n",
                "    def call(self, y_true, y_pred):\n",
                "        y_true = tf.cast(y_true, tf.float32)\n",
                "        if self.from_logits:\n",
                "            y_pred = tf.nn.softmax(y_pred)\n",
                "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
                "        ce     = -y_true * tf.math.log(y_pred)\n",
                "        weight = self.alpha * tf.pow(1.0 - y_pred, self.gamma)\n",
                "        return tf.reduce_sum(weight * ce, axis=-1)\n",
                "\n",
                "    def get_config(self):\n",
                "        return {\"gamma\": self.gamma, \"alpha\": self.alpha, \"from_logits\": self.from_logits, \"name\": self.name}\n",
                "\n",
                "@keras.saving.register_keras_serializable(package=\"custom\")\n",
                "class DenseNetPreprocess(tf.keras.layers.Layer):\n",
                "    def call(self, x):\n",
                "        return tf.keras.applications.densenet.preprocess_input(x)\n",
                "    def get_config(self):\n",
                "        return {}\n",
                "\n",
                "LOSS_FN = FocalLoss(gamma=GAMMA, alpha=ALPHA) if USE_FOCAL_LOSS else \"categorical_crossentropy\"\n",
                "print(\"Loss Function:\", LOSS_FN.name if hasattr(LOSS_FN, 'name') else LOSS_FN)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fcd69dbd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 6) Model Architecture (DenseNet201 + SE Block)\n",
                "# ----------------------------\n",
                "def se_block(x, ratio=16, name=\"se\"):\n",
                "    c  = int(x.shape[-1])\n",
                "    se = tf.keras.layers.GlobalAveragePooling2D(name=f\"{name}_gap\")(x)\n",
                "    se = tf.keras.layers.Dense(max(1, c // ratio), activation=\"relu\",    name=f\"{name}_fc1\")(se)\n",
                "    se = tf.keras.layers.Dense(c,                  activation=\"sigmoid\", name=f\"{name}_fc2\")(se)\n",
                "    se = tf.keras.layers.Reshape((1, 1, c),                              name=f\"{name}_reshape\")(se)\n",
                "    return tf.keras.layers.Multiply(name=f\"{name}_scale\")([x, se])\n",
                "\n",
                "def build_densenet201_se(num_classes, dropout=DROPOUT, se_ratio=SE_RATIO, name=\"DenseNet201_SE\"):\n",
                "    backbone = tf.keras.applications.DenseNet201(\n",
                "        include_top=False, weights=\"imagenet\",\n",
                "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
                "    )\n",
                "    backbone.trainable = False\n",
                "\n",
                "    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
                "    x   = DenseNetPreprocess(name=\"densenet_preprocess\")(inp)\n",
                "    x   = backbone(x, training=False)\n",
                "    x   = se_block(x, ratio=se_ratio, name=\"se_block\")\n",
                "    x   = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
                "    x   = tf.keras.layers.Dropout(dropout)(x)\n",
                "    out = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
                "    return tf.keras.Model(inp, out, name=name)\n",
                "\n",
                "model = build_densenet201_se(num_classes)\n",
                "model.compile(optimizer=tf.keras.optimizers.Adam(LR1), loss=LOSS_FN, metrics=[\"accuracy\"])\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "da4b1224",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 7) Class Weights\n",
                "# ----------------------------\n",
                "classes_idx  = train_df[\"label\"].map(class_to_idx).values\n",
                "cw           = compute_class_weight(class_weight=\"balanced\", classes=np.unique(classes_idx), y=classes_idx)\n",
                "class_weight = {i: float(w) for i, w in enumerate(cw)}\n",
                "print(\"Class Weights:\", class_weight)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "097c14ae",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 8) Train & Eval Plot Helper\n",
                "# ----------------------------\n",
                "def plot_history(history, stage_name=\"Training\"):\n",
                "    acc      = history.history['accuracy']\n",
                "    val_acc  = history.history['val_accuracy']\n",
                "    loss     = history.history['loss']\n",
                "    val_loss = history.history['val_loss']\n",
                "    epochs   = range(1, len(acc) + 1)\n",
                "\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
                "    fig.suptitle(f\"Train Evaluation ‚Äî {stage_name}\", fontsize=14, fontweight='bold')\n",
                "\n",
                "    ax1.plot(epochs, acc,     'bo-', label='Train Accuracy')\n",
                "    ax1.plot(epochs, val_acc, 'ro-', label='Val Accuracy')\n",
                "    ax1.set_title('Accuracy'); ax1.set_xlabel('Epoch'); ax1.set_ylabel('Accuracy')\n",
                "    ax1.legend(); ax1.grid(True, alpha=0.3)\n",
                "\n",
                "    ax2.plot(epochs, loss,     'bo-', label='Train Loss')\n",
                "    ax2.plot(epochs, val_loss, 'ro-', label='Val Loss')\n",
                "    ax2.set_title('Loss'); ax2.set_xlabel('Epoch'); ax2.set_ylabel('Loss')\n",
                "    ax2.legend(); ax2.grid(True, alpha=0.3)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c532eaaf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 9) STAGE 1: Train Head Only\n",
                "# ----------------------------\n",
                "checkpoint_path = os.path.join(OUTPUT_DIR, \"best_stage1.keras\")\n",
                "\n",
                "callbacks_stage1 = [\n",
                "    tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
                "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1)\n",
                "]\n",
                "\n",
                "print(\"\\nüöÄ Starting Stage 1 Training (Head Only)...\")\n",
                "history1 = model.fit(\n",
                "    train_ds, validation_data=val_ds,\n",
                "    epochs=EPOCHS_STAGE1, callbacks=callbacks_stage1, class_weight=class_weight\n",
                ")\n",
                "plot_history(history1, \"Stage 1 ‚Äî Head Training\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43e920b7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 10) STAGE 2: Fine Tuning\n",
                "# ----------------------------\n",
                "model = tf.keras.models.load_model(\n",
                "    checkpoint_path,\n",
                "    custom_objects={\"FocalLoss\": FocalLoss, \"DenseNetPreprocess\": DenseNetPreprocess}\n",
                ")\n",
                "\n",
                "backbone = model.layers[2]\n",
                "backbone.trainable = True\n",
                "\n",
                "for layer in backbone.layers[:-UNFREEZE_LAST]:\n",
                "    layer.trainable = False\n",
                "for layer in backbone.layers:\n",
                "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
                "        layer.trainable = False\n",
                "\n",
                "model.compile(optimizer=tf.keras.optimizers.Adam(LR2), loss=LOSS_FN, metrics=[\"accuracy\"])\n",
                "\n",
                "final_checkpoint_path = os.path.join(OUTPUT_DIR, \"densenet201_se_final.keras\")\n",
                "\n",
                "callbacks_stage2 = [\n",
                "    tf.keras.callbacks.ModelCheckpoint(final_checkpoint_path, monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
                "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True, verbose=1),\n",
                "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1)\n",
                "]\n",
                "\n",
                "print(f\"\\nüöÄ Starting Stage 2 Fine-Tuning (Unfreezing last {UNFREEZE_LAST} layers)...\")\n",
                "history2 = model.fit(\n",
                "    train_ds, validation_data=val_ds,\n",
                "    epochs=EPOCHS_STAGE2, callbacks=callbacks_stage2, class_weight=class_weight\n",
                ")\n",
                "plot_history(history2, \"Stage 2 ‚Äî Fine Tuning\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "351192f9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 11) Evaluasi: Confusion Matrix & Report\n",
                "# ----------------------------\n",
                "best_model = tf.keras.models.load_model(\n",
                "    final_checkpoint_path,\n",
                "    custom_objects={\"FocalLoss\": FocalLoss, \"DenseNetPreprocess\": DenseNetPreprocess}\n",
                ")\n",
                "\n",
                "print(\"\\nüìä Generating Evaluation Metrics...\")\n",
                "\n",
                "y_pred, y_true = [], []\n",
                "for bx, by in val_ds:\n",
                "    probs = best_model.predict(bx, verbose=0)\n",
                "    y_pred.extend(np.argmax(probs, axis=1))\n",
                "    y_true.extend(np.argmax(by.numpy(), axis=1))\n",
                "\n",
                "print(\"\\n‚úÖ Classification Report:\")\n",
                "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
                "\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "plt.figure(figsize=(11, 8))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
                "plt.xlabel('Predicted'); plt.ylabel('Ground Truth')\n",
                "plt.title('Confusion Matrix ‚Äî Validation Set')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4e83b055",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 12) Save Model\n",
                "# ----------------------------\n",
                "print(\"\\nüíæ Model saved successfully at:\")\n",
                "print(f\"   - {final_checkpoint_path}\")\n",
                "print(\"   (Gunakan file ini untuk inference atau deployment)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "inf_md_001",
            "metadata": {},
            "source": [
                "---\n",
                "## üîç 13) Inference Helper\n",
                "\n",
                "Bagian ini digunakan untuk **menguji model** pada gambar baru setelah proses training selesai.\n",
                "**Tidak perlu training ulang** ‚Äî cukup load model yang sudah tersimpan dan jalankan prediksi.\n",
                "\n",
                "### Cara menjalankan:\n",
                "\n",
                "| Urutan | Cell | Fungsi |\n",
                "|:---:|---|---|\n",
                "| **1** | **Cell 13a** | Load semua library + model + class names |\n",
                "| **2** | **Cell 13b** | Prediksi satu gambar dari folder `test_images/` (di luar dataset) |\n",
                "| **3** | **Cell 13c** | Prediksi semua gambar di folder `test_images/` ‚Üí simpan hasil ke CSV |\n",
                "\n",
                "> ‚ö†Ô∏è **Cell 13a harus dijalankan pertama.** Letakkan gambar uji di folder `test_images/` (bukan di dalam `paddy-disease-classification/`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "inf_code_001",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 13a) Load Model + Library untuk Inference\n",
                "# WAJIB dijalankan pertama sebelum Cell 13b / 13c\n",
                "# ----------------------------\n",
                "import os\n",
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as mpatches\n",
                "import tensorflow as tf\n",
                "import keras\n",
                "\n",
                "# ============================================================\n",
                "# CONFIG ‚Äî Sesuaikan path di sini jika perlu\n",
                "# ============================================================\n",
                "MODEL_PATH       = \"model/outputs_densenet201_se_standalone/densenet201_se_final.keras\"\n",
                "CLASS_NAMES_PATH = \"model/outputs_densenet201_se_standalone/class_names.json\"\n",
                "\n",
                "# Folder gambar uji (di LUAR folder paddy-disease-classification)\n",
                "CUSTOM_TEST_DIR  = \"test_images\"\n",
                "\n",
                "IMG_SIZE = (224, 224)\n",
                "# ============================================================\n",
                "\n",
                "# Re-register custom components (wajib agar load_model berhasil)\n",
                "@keras.saving.register_keras_serializable(package=\"custom\")\n",
                "class FocalLoss(tf.keras.losses.Loss):\n",
                "    def __init__(self, gamma=2.0, alpha=0.25, from_logits=False, name=\"focal_loss\"):\n",
                "        super().__init__(name=name)\n",
                "        self.gamma = gamma; self.alpha = alpha; self.from_logits = from_logits\n",
                "    def call(self, y_true, y_pred):\n",
                "        if self.from_logits: y_pred = tf.nn.softmax(y_pred)\n",
                "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
                "        ce = -tf.cast(y_true, tf.float32) * tf.math.log(y_pred)\n",
                "        return tf.reduce_sum(self.alpha * tf.pow(1.0 - y_pred, self.gamma) * ce, axis=-1)\n",
                "    def get_config(self):\n",
                "        return {\"gamma\": self.gamma, \"alpha\": self.alpha, \"from_logits\": self.from_logits, \"name\": self.name}\n",
                "\n",
                "@keras.saving.register_keras_serializable(package=\"custom\")\n",
                "class DenseNetPreprocess(tf.keras.layers.Layer):\n",
                "    def call(self, x): return tf.keras.applications.densenet.preprocess_input(x)\n",
                "    def get_config(self): return {}\n",
                "\n",
                "CUSTOM_OBJ = {\"FocalLoss\": FocalLoss, \"DenseNetPreprocess\": DenseNetPreprocess}\n",
                "\n",
                "# Load model\n",
                "print(f\"[INFO] Loading model dari: {MODEL_PATH}\")\n",
                "inf_model = tf.keras.models.load_model(MODEL_PATH, custom_objects=CUSTOM_OBJ)\n",
                "print(\"[INFO] ‚úÖ Model berhasil di-load!\")\n",
                "\n",
                "# Load class names\n",
                "with open(CLASS_NAMES_PATH, \"r\") as f:\n",
                "    class_names_inf = json.load(f)\n",
                "\n",
                "print(f\"[INFO] Classes ({len(class_names_inf)}): {class_names_inf}\")\n",
                "print(f\"[INFO] Folder gambar uji : '{CUSTOM_TEST_DIR}'\")\n",
                "print(\"\\n‚úÖ Siap untuk inference. Jalankan Cell 13b atau 13c.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "inf_md_002",
            "metadata": {},
            "source": [
                "### 13b. Prediksi Satu Gambar\n",
                "\n",
                "Letakkan gambar daun padi Anda di folder **`test_images/`** (sejajar dengan notebook ini, di luar folder dataset).\n",
                "\n",
                "Lalu ubah nama file di variabel `IMAGE_FILE` sesuai nama gambar Anda.\n",
                "\n",
                "**Contoh struktur folder:**\n",
                "```\n",
                "Model DenseNet-201/\n",
                "‚îú‚îÄ‚îÄ test_images/          ‚Üê Taruh gambar di sini\n",
                "‚îÇ   ‚îú‚îÄ‚îÄ image.png\n",
                "‚îÇ   ‚îî‚îÄ‚îÄ foto_daun.jpg\n",
                "‚îú‚îÄ‚îÄ paddy-disease-classification/   ‚Üê Dataset Kaggle (jangan diubah)\n",
                "‚îî‚îÄ‚îÄ densenet201_se_standalone_FIX_keras3.ipynb\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "inf_code_002",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 13b) Prediksi SATU Gambar\n",
                "# ----------------------------\n",
                "def predict_single_image(model, class_names, image_path, img_size=(224, 224)):\n",
                "    import matplotlib.patches as mpatches\n",
                "\n",
                "    if not os.path.exists(image_path):\n",
                "        raise FileNotFoundError(\n",
                "            f\"‚ùå Gambar tidak ditemukan: '{image_path}'\\n\"\n",
                "            f\"   Pastikan file ada di folder '{CUSTOM_TEST_DIR}/'\\n\"\n",
                "            f\"   Contoh: taruh gambar di '{CUSTOM_TEST_DIR}/image.png' lalu set IMAGE_FILE = 'image.png'\"\n",
                "        )\n",
                "\n",
                "    # Preprocessing\n",
                "    img_raw     = tf.io.read_file(image_path)\n",
                "    img         = tf.image.decode_image(img_raw, channels=3, expand_animations=False)\n",
                "    img_display = img.numpy().astype('uint8')\n",
                "    img         = tf.image.resize(img, img_size, method=\"bilinear\")\n",
                "    img         = tf.cast(img, tf.float32)\n",
                "    img         = tf.expand_dims(img, axis=0)\n",
                "\n",
                "    # Inference\n",
                "    probs      = model.predict(img, verbose=0)[0]\n",
                "    pred_idx   = int(np.argmax(probs))\n",
                "    pred_label = class_names[pred_idx]\n",
                "    confidence = float(probs[pred_idx])\n",
                "\n",
                "    # Visualisasi\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    fig.suptitle(\"Hasil Prediksi DenseNet201-SE\", fontsize=14, fontweight='bold')\n",
                "\n",
                "    ax1.imshow(img_display)\n",
                "    ax1.axis('off')\n",
                "    color = '#27ae60' if confidence >= 0.80 else ('#e67e22' if confidence >= 0.50 else '#e74c3c')\n",
                "    ax1.set_title(\n",
                "        f\"üè∑Ô∏è  {pred_label.replace('_', ' ').title()}\\nConfidence: {confidence*100:.2f}%\",\n",
                "        fontsize=13, fontweight='bold', color=color\n",
                "    )\n",
                "\n",
                "    colors = ['#27ae60' if i == pred_idx else '#3498db' for i in range(len(class_names))]\n",
                "    bars   = ax2.barh(class_names, probs * 100, color=colors, edgecolor='white', height=0.6)\n",
                "    ax2.set_xlabel('Probability (%)', fontsize=11)\n",
                "    ax2.set_title('Distribusi Probabilitas Semua Kelas', fontsize=11)\n",
                "    ax2.set_xlim(0, 105)\n",
                "    for bar, val in zip(bars, probs):\n",
                "        ax2.text(val * 100 + 0.8, bar.get_y() + bar.get_height() / 2,\n",
                "                 f'{val*100:.1f}%', va='center', fontsize=9)\n",
                "\n",
                "    green_patch = mpatches.Patch(color='#27ae60', label='Predicted Class')\n",
                "    blue_patch  = mpatches.Patch(color='#3498db', label='Other Classes')\n",
                "    ax2.legend(handles=[green_patch, blue_patch], loc='lower right')\n",
                "    ax2.grid(axis='x', alpha=0.3)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "    print(f\"\\nüéØ Hasil Prediksi:\")\n",
                "    print(f\"   Kelas      : {pred_label}\")\n",
                "    print(f\"   Confidence : {confidence*100:.2f}%\")\n",
                "    return pred_label, confidence, probs\n",
                "\n",
                "\n",
                "# ===================================================\n",
                "# GANTI nama file gambar di sini\n",
                "# Gambar harus ada di folder 'test_images/'\n",
                "IMAGE_FILE = \"image.png\"   # ‚Üê ganti sesuai nama file Anda\n",
                "# ===================================================\n",
                "\n",
                "IMAGE_PATH = os.path.join(CUSTOM_TEST_DIR, IMAGE_FILE)\n",
                "print(f\"[INFO] Memproses gambar: {IMAGE_PATH}\")\n",
                "\n",
                "pred_label, confidence, all_probs = predict_single_image(inf_model, class_names_inf, IMAGE_PATH)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "inf_md_003",
            "metadata": {},
            "source": [
                "### 13c. Prediksi Semua Gambar di `test_images/` ‚Üí CSV\n",
                "\n",
                "Cell ini akan memproses **semua file gambar** (`.jpg`, `.jpeg`, `.png`) yang ada di folder `test_images/` secara batch, lalu menyimpan hasilnya ke file CSV.\n",
                "\n",
                "**Format output CSV:**\n",
                "```\n",
                "filename,predicted_label,confidence\n",
                "image.png,blast,0.9732\n",
                "foto_daun.jpg,normal,0.8811\n",
                "```\n",
                "File disimpan di: `test_images/prediction_results.csv`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "inf_code_003",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ----------------------------\n",
                "# 13c) Prediksi Semua Gambar di test_images/ ‚Üí CSV\n",
                "# ----------------------------\n",
                "def predict_all_in_folder(model, class_names, test_dir, img_size=(224, 224), batch_size=16):\n",
                "    \"\"\"\n",
                "    Prediksi semua gambar (.jpg/.jpeg/.png) di folder test_dir.\n",
                "    Hasil disimpan ke CSV di dalam folder yang sama.\n",
                "    \"\"\"\n",
                "    # Scan semua file gambar\n",
                "    valid_ext = ('.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG')\n",
                "    all_files = [f for f in os.listdir(test_dir) if f.endswith(valid_ext)]\n",
                "\n",
                "    if not all_files:\n",
                "        print(f\"[WARN] ‚ùå Tidak ada gambar di folder '{test_dir}'.\")\n",
                "        print(f\"        Taruh file .jpg/.png ke folder tersebut lalu jalankan lagi.\")\n",
                "        return None\n",
                "\n",
                "    print(f\"[INFO] Ditemukan {len(all_files)} gambar di '{test_dir}'\")\n",
                "    all_files = sorted(all_files)\n",
                "    test_paths = [os.path.join(test_dir, f) for f in all_files]\n",
                "\n",
                "    # Build tf.data pipeline\n",
                "    def load_img(path):\n",
                "        raw  = tf.io.read_file(path)\n",
                "        img  = tf.image.decode_image(raw, channels=3, expand_animations=False)\n",
                "        img  = tf.image.resize(img, img_size, method=\"bilinear\")\n",
                "        return tf.cast(img, tf.float32)\n",
                "\n",
                "    test_ds = (\n",
                "        tf.data.Dataset.from_tensor_slices(test_paths)\n",
                "        .map(load_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
                "        .batch(batch_size)\n",
                "        .prefetch(tf.data.AUTOTUNE)\n",
                "    )\n",
                "\n",
                "    # Inference\n",
                "    print(\"[INFO] Menjalankan prediksi...\")\n",
                "    all_probs   = np.concatenate([model.predict(batch, verbose=0) for batch in test_ds], axis=0)\n",
                "    pred_idx    = np.argmax(all_probs, axis=1)\n",
                "    pred_labels = [class_names[i] for i in pred_idx]\n",
                "    confidences = [float(all_probs[i, pred_idx[i]]) for i in range(len(pred_idx))]\n",
                "\n",
                "    # Simpan CSV\n",
                "    result_df = pd.DataFrame({\n",
                "        \"filename\"        : all_files,\n",
                "        \"predicted_label\" : pred_labels,\n",
                "        \"confidence\"      : [round(c, 4) for c in confidences]\n",
                "    })\n",
                "\n",
                "    out_csv = os.path.join(test_dir, \"prediction_results.csv\")\n",
                "    result_df.to_csv(out_csv, index=False)\n",
                "    print(f\"[INFO] ‚úÖ Hasil disimpan di: {out_csv}\")\n",
                "\n",
                "    # Visualisasi distribusi\n",
                "    pred_dist = pd.Series(pred_labels).value_counts().sort_index()\n",
                "    plt.figure(figsize=(10, 4))\n",
                "    pred_dist.plot(kind='bar', color='#3498db', edgecolor='white', width=0.6)\n",
                "    plt.title('Distribusi Prediksi', fontsize=13, fontweight='bold')\n",
                "    plt.xlabel('Kelas Penyakit'); plt.ylabel('Jumlah Gambar')\n",
                "    plt.xticks(rotation=30, ha='right')\n",
                "    plt.grid(axis='y', alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "    print(\"\\nHasil Prediksi:\")\n",
                "    print(result_df.to_string(index=False))\n",
                "    return result_df\n",
                "\n",
                "\n",
                "# Jalankan batch prediction\n",
                "result_df = predict_all_in_folder(\n",
                "    model       = inf_model,\n",
                "    class_names = class_names_inf,\n",
                "    test_dir    = CUSTOM_TEST_DIR\n",
                ")\n",
                "\n",
                "if result_df is not None:\n",
                "    result_df.head(20)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 5,
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}